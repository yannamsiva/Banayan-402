<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Automate your Quarkus deployment using Ansible</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/06/01/automate-your-quarkus-deployment-using-ansible" /><author><name>Romain Pelisse</name></author><id>0e1a45d5-12ee-44f5-9e84-7181a36393b2</id><updated>2023-06-01T07:00:00Z</updated><published>2023-06-01T07:00:00Z</published><summary type="html">&lt;p&gt;In this article, we’ll explain how to use Ansible to build and deploy a &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus application&lt;/a&gt;. &lt;a href="https://www.redhat.com/en/topics/cloud-native-apps/what-is-quarkus"&gt;Quarkus&lt;/a&gt; is an exciting, lightweight &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; development framework designed for cloud and &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; deployments, and &lt;a href="https://developers.redhat.com/products/ansible/"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;  is one of the most popular &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; tools and a star product from Red Hat.&lt;/p&gt; &lt;h2&gt;Set up your Ansible environment&lt;/h2&gt; &lt;p&gt;Before discussing how to automate a Quarkus application deployment using Ansible, we need to ensure the prerequisites are in place. First, you have to install Ansible on your development environment. On a Fedora or a &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; machine, this is achieved easily utilizing the &lt;a href="https://docs.fedoraproject.org/en-US/quick-docs/dnf/"&gt;dnf package manager&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ dnf install ansible-core&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The only other requirement is to install the Ansible collection dedicated to Quarkus:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-galaxy collection install middleware_automation.quarkus&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is all you need to prepare the Ansible control machine (the name given to the machine executing Ansible).&lt;/p&gt; &lt;p&gt;Generally, the &lt;a href="https://docs.ansible.com/ansible/latest/network/getting_started/basic_concepts.html#control-node"&gt;control node&lt;/a&gt; is used to set up other systems that are designated under the name &lt;strong&gt;targets&lt;/strong&gt;. For the purpose of this tutorial, and for simplicity's sake, we are going to utilize the same system for both the control node and our (only) target. This will make it easier to reproduce the content of this article on a single development machine.&lt;/p&gt; &lt;p&gt;Note that you don’t need to set up any kind of Java development environment, because the Ansible collection will take care of that.&lt;/p&gt; &lt;p&gt;The Ansible collection dedicated to Quarkus is a community project, and it’s not supported by Red Hat. However, both Quarkus and Ansible are Red Hat products and thus fully supported. The Quarkus collection might be supported at some point in the future, but is not as the time of the writing of this article.&lt;/p&gt; &lt;h3&gt;Inventory file&lt;/h3&gt; &lt;p&gt;Before we can execute Ansible, we need to provide to the tool an &lt;a href="https://docs.ansible.com/ansible/latest/inventory_guide/intro_inventory.html"&gt;inventory&lt;/a&gt; of the targets. There are many ways to achieve that, but the simplest solution for a tutorial such as this one is to write up an inventory file of our own.&lt;/p&gt; &lt;p&gt;As mentioned above, we are going to use the same host for both the controller and the target, so the inventory file has only one host. Here again, for simplicity's sake, this machine is going to be the localhost: &lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cat inventory [all] localhost ansible_connection=local&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Refer to the Ansible documentation for more information on &lt;a href="https://docs.ansible.com/ansible/latest/inventory_guide/intro_inventory.html"&gt;Ansible inventory&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Build and deploy the app with Ansible&lt;/h2&gt; &lt;p&gt;For this demonstration, we are going to utilize one of the sample applications provided as part of the &lt;a href="https://github.com/quarkusio/quarkus-quickstarts/tree/main/getting-started"&gt;Quarkus quick starts project&lt;/a&gt;. We will use Ansible to build and deploy the &lt;strong&gt;getting started&lt;/strong&gt; application.&lt;/p&gt; &lt;p&gt;All we need to provide to Ansible is the application name, repository URL, and the destination folder, where to deploy the application on the target. Because of the directory structure of the Quarkus quick start, containing several projects, we'll also need to specify the directory containing the source code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-playbook -i inventory middleware_automation.quarkus.playbook \ -e app_name='optaplanner-quickstart' \ -e quarkus_app_source_folder='optaplanner-quickstart' \ -e quarkus_path_to_folder_to_deploy=/opt/optplanner \ -e quarkus_app_repo_url='https://github.com/quarkusio/quarkus-quickstarts.git'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Below is the output of this command :&lt;/p&gt; &lt;pre&gt; PLAY [Build and deploy a Quarkus app using Ansible] **************************** TASK [Gathering Facts] ********************************************************* ok: [localhost] TASK [Build the Quarkus from https://github.com/quarkusio/quarkus-quickstarts.git.] *** TASK [middleware_automation.quarkus.quarkus : Ensure required parameters are provided.] *** ok: [localhost] TASK [middleware_automation.quarkus.quarkus : Define path to mvnw script.] ***** ok: [localhost] TASK [middleware_automation.quarkus.quarkus : Ensure that builder host localhost has appropriate JDK installed: java-17-openjdk] *** changed: [localhost] TASK [middleware_automation.quarkus.quarkus : Delete previous workdir (if requested).] *** ok: [localhost] TASK [middleware_automation.quarkus.quarkus : Ensure app workdir exists: /tmp/workdir] *** changed: [localhost] TASK [middleware_automation.quarkus.quarkus : Checkout the application source code.] *** changed: [localhost] TASK [middleware_automation.quarkus.quarkus : Build the App using Maven] ******* ok: [localhost] TASK [middleware_automation.quarkus.quarkus : Display build application log] *** skipping: [localhost] TASK [Deploy Quarkus app on target.] ******************************************* TASK [middleware_automation.quarkus.quarkus : Ensure required parameters are provided.] *** ok: [localhost] TASK [middleware_automation.quarkus.quarkus : Ensure requirements on target system are fullfilled.] *** included: /root/.ansible/collections/ansible_collections/middleware_automation/quarkus/roles/quarkus/tasks/deploy/prereqs.yml for localhost TASK [middleware_automation.quarkus.quarkus : Ensure required OpenJDK is installed on target.] *** skipping: [localhost] TASK [middleware_automation.quarkus.quarkus : Ensure Quarkus system group exists on target system] *** changed: [localhost] TASK [middleware_automation.quarkus.quarkus : Ensure Quarkus user exists on target system.] *** changed: [localhost] TASK [middleware_automation.quarkus.quarkus : Ensure deployement directory exits: /opt/optplanner.] *** changed: [localhost] TASK [middleware_automation.quarkus.quarkus : Set Quarkus app source dir (if not defined).] *** ok: [localhost] TASK [middleware_automation.quarkus.quarkus : Deploy application as a systemd service on target system.] *** included: /root/.ansible/collections/ansible_collections/middleware_automation/quarkus/roles/quarkus/tasks/deploy/service.yml for localhost TASK [middleware_automation.quarkus.quarkus : Deploy application from to target system] *** ok: [localhost] TASK [middleware_automation.quarkus.quarkus : Deploy Systemd configuration for Quarkus app] *** changed: [localhost] TASK [middleware_automation.quarkus.quarkus : Perform daemon-reload to ensure the changes are picked up] *** ok: [localhost] TASK [middleware_automation.quarkus.quarkus : Ensure Quarkus app service is running.] *** changed: [localhost] TASK [middleware_automation.quarkus.quarkus : Ensure firewalld configuration is appropriate (if requested).] *** skipping: [localhost] PLAY RECAP ********************************************************************* localhost : ok=19 changed=8 unreachable=0 failed=0 skipped=3 rescued=0 ignored=0 &lt;/pre&gt; &lt;p&gt;As you can see, the Ansible collection for Quarkus does all the heavy lifting for us: its content takes care of checking out the source code from GitHub and builds the application. It also ensures the system used for this step has the required OpenJDK installed on the target machine.&lt;/p&gt; &lt;p&gt;Once the application is successfully built, the collection takes care of the deployment. Here again, it checks that the appropriate OpenJDK is available on the target system. Then, it verifies that the required user and group exist on the target and if not, creates them. This is recommended mostly to be able to run the Quarkus application with a regular user, rather than with the root account.&lt;/p&gt; &lt;p&gt;With those requirements in place, the jars produced during the build phase are copied over to the target, along with the required configuration for the application integration into &lt;a href="https://developers.redhat.com/cheat-sheets/systemd-commands-cheat-sheet"&gt;systemd&lt;/a&gt; as a service. Any change to the systemd configuration requires reloading its daemon, which the collection ensures will happen whenever it is needed. With all of that in place, the collection starts the service itself.&lt;/p&gt; &lt;h2&gt;Validate the execution results &lt;/h2&gt; &lt;p&gt;Let’s take a minute to verify that all went well and that the service is indeed running:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# systemctl status optaplanner-quickstart.service ● optaplanner-quickstart.service - A Quarkus service named optaplanner-quickstart Loaded: loaded (/usr/lib/systemd/system/optaplanner-quickstart.service; enabled; vendor preset: disabled) Active: active (running) since Wed 2023-04-26 09:40:13 UTC; 3h 19min ago Main PID: 934 (java) CGroup: /system.slice/optaplanner-quickstart.service └─934 /usr/bin/java -jar /opt/optplanner/quarkus-run.jar Apr 26 09:40:13 be44b3acb1f3 systemd[1]: Started A Quarkus service named optaplanner-quickstart. Apr 26 09:40:14 be44b3acb1f3 java[934]: __ ____ __ _____ ___ __ ____ ______ Apr 26 09:40:14 be44b3acb1f3 java[934]: --/ __ \/ / / / _ | / _ \/ //_/ / / / __/ Apr 26 09:40:14 be44b3acb1f3 java[934]: -/ /_/ / /_/ / __ |/ , _/ ,&lt; / /_/ /\ \ Apr 26 09:40:14 be44b3acb1f3 java[934]: --\___\_\____/_/ |_/_/|_/_/|_|\____/___/ Apr 26 09:40:14 be44b3acb1f3 java[934]: 2023-04-26 09:40:14,843 INFO [io.quarkus] (main) optaplanner-quickstart 1.0.0-SNAPSHOT on JVM (powered by Quarkus 2.16.6.Final) started in 1.468s. Listening on: http://0.0.0.0:8080 Apr 26 09:40:14 be44b3acb1f3 java[934]: 2023-04-26 09:40:14,848 INFO [io.quarkus] (main) Profile prod activated. Apr 26 09:40:14 be44b3acb1f3 java[934]: 2023-04-26 09:40:14,848 INFO [io.quarkus] (main) Installed features: [agroal, cdi, hibernate-orm, hibernate-orm-panache, hibernate-orm-rest-data-panache, jdbc-h2, narayana-jta, optaplanner, optaplanner-jackson, resteasy-reactive, resteasy-reactive-jackson, resteasy-reactive-links, smallrye-context-propagation, vertx, webjars-locator] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Having the service running is certainly good, but it does not guarantee by itself that the application is available. To double-check, we can simply confirm the accessibility of the application by connecting to it:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# curl -I http://localhost:8080/ HTTP/1.1 200 OK accept-ranges: bytes content-length: 8533 cache-control: public, immutable, max-age=86400 last-modified: Wed, 26 Apr 2023 10:00:18 GMT date: Wed, 26 Apr 2023 13:00:19 GMT&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Writing up a playbook&lt;/h2&gt; &lt;p&gt;The default playbook provided with the Ansible collection for Quarkus is quite handy and allows you to bootstrap your automation with a single command. However, most likely, you’ll need to write your own playbook so you can add automation required around the deployment of your Quarkus app.&lt;/p&gt; &lt;p&gt;Here is the content of the playbook provided with the collection that you can simply use as a base for your own:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: "Build and deploy a Quarkus app using Ansible" hosts: all gather_facts: false vars: quarkus_app_repo_url: 'https://github.com/quarkusio/quarkus-quickstarts.git' app_name: optaplanner-quickstart' quarkus_app_source_folder: 'optaplanner-quickstart' quarkus_path_to_folder_to_deploy: '/opt/optaplanner' pre_tasks: - name: "Build the Quarkus from {{ quarkus_app_repo_url }}." ansible.builtin.include_role: name: quarkus tasks_from: build.yml tasks: - name: "Deploy Quarkus app on target." ansible.builtin.include_role: name: quarkus tasks_from: deploy.yml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To run this playbook, you again use the &lt;code&gt;ansible-playbook &lt;/code&gt;command, but providing the path to the playbook:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-playbook -i inventory playbook.yml&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Thanks to the Ansible collection for Quarkus, the work needed to automate the deployment of a Quarkus application is minimal. The collection takes care of most of the heavy lifting and allows its user to focus on the automation needs specific to their application and business needs.&lt;/p&gt; &lt;p&gt;Explore other Ansible tutorials on Red Hat Developer:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2023/04/04/update-and-upgrade-jboss-eap-ansible"&gt;Update and upgrade JBoss EAP with Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2023/02/20/automate-your-sso-ansible-and-keycloak"&gt;Automate your SSO with Ansible and Keycloak&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/12/22/automate-jboss-web-server-deployment-red-hat-certified-content-collection-jws"&gt;Automate JBoss Web Server deployment with the Red Hat Certified Content Collection for JWS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/20/deploy-keycloak-single-sign-ansible"&gt;Deploy Keycloak single sign-on with Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/03/21/deploy-infinispan-automatically-ansible"&gt;Deploy Infinispan automatically with Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/02/08/automate-and-deploy-jboss-eap-cluster-ansible"&gt;Automate and deploy a JBoss EAP cluster with Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/09/28/set-modcluster-red-hat-jboss-web-server-ansible"&gt;Set up mod_cluster for Red Hat JBoss Web Server with Ansible&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/06/01/automate-your-quarkus-deployment-using-ansible" title="Automate your Quarkus deployment using Ansible"&gt;Automate your Quarkus deployment using Ansible&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Romain Pelisse</dc:creator><dc:date>2023-06-01T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - June, 1st 2023</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2023-06-01.html" /><category term="quarkus" /><category term="java" /><category term="jee" /><category term="micro-profile" /><category term="wildfly" /><category term="ansible" /><category term="kogito" /><category term="keycloak" /><author><name>Romain Pelisse 2023-06-01</name><uri>https://www.jboss.org/people/romain-pelisse 2023-06-01</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2023-06-01.html</id><updated>2023-06-01T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, java, jee, micro-profile, wildfly, ansible, kogito, keycloak"&gt; &lt;h1&gt;This Week in JBoss - June, 1st 2023&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;First and foremost! Dear readers, we want our opinion about the content of the editorial. We know filling up poll can be annoying, so you made as simple as possible…​ Please answer our &lt;strong&gt;one question only&lt;/strong&gt; &lt;a href="https://framadate.org/XbAltuQw4kQDY9At"&gt;poll on our editorial&lt;/a&gt;! Thanks!&lt;/p&gt; &lt;p&gt;Two of the biggest stars of the JBoss ecosystem are for sure Quarkus and, of course, Wildfly, the application server that used to be called, well, JBoss! The last weeks have seen a lot of interesting content and news around those two projects, so we are going to focus this editorial on them. Buckle up, there is a ton of passionating stuff coming your way!&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_quarkus"&gt;Quarkus&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;The last two weeks have seen again quite a few interesting news, and content released, for the "Supersonic Subatomic Java" framework Quarkus! On top on the &lt;a href="https://quarkus.io/blog/quarkus-3-1-0-final-released/"&gt;3.1.0.Final release&lt;/a&gt; and the &lt;a href="https://quarkus.io/blog/quarkus-3-0-4-final-released/"&gt;3.0.4.Final release&lt;/a&gt; a new guide to migrate to those versions have been published: &lt;a href="https://quarkus.io/blog/quarkus-3-upgrade/"&gt;Migration to Quarkus 3.0 is a breeze&lt;/a&gt;! No reason to stay behind now, jump on board of Quarkus 3!&lt;/p&gt; &lt;p&gt;I’m also happy to mention that my guide on &lt;a href="https://quarkus.io/guides/ansible"&gt;Automate Quarkus deployment with Ansible&lt;/a&gt; has been added to the website. Hope it helps Quarkus user who wants to automate their deployment!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_be_the_boss_of_wildfly"&gt;Be The Boss of Wildfly&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;In the unlikely case, you have never heard of this website, Master The Boss, that has been around since forever, you have a chance to discover today. It has been publishing content on JBoss for over a decade and the articles published in the last week are pretty example of the site’s richness.&lt;/p&gt; &lt;p&gt;First one we wanted to mention is &lt;a href="https://www.mastertheboss.com/keycloak/google-social-login-with-keycloak/"&gt;KeyCloak Social Login Step-by-Step guide&lt;/a&gt;. Keycloak is popular SSO software and social login is certainly worth a mention. Then come two articles more focused on Wildfly. The first one is &lt;a href="https://www.mastertheboss.com/eclipse/eclipse-microservices/microprofile-lra-a-comprehensive-guide/"&gt;MicroProfile LRA: A Comprehensive Guide&lt;/a&gt; and the second is &lt;a href="https://www.mastertheboss.com/eclipse/jboss-tools/using-visual-studio-to-develop-and-manage-wildfly/"&gt;Using Visual Studio to develop and manage WildFly&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;As always the JBoss world is a thriving place thus the last got her fair share of releases:&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-3-1-0-final-released/"&gt;Quarkus 3.1.0.Final released&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-3-0-4-final-released/"&gt;Quarkus 3.0.4.Final&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org//news/2023/05/23/WildFly2801-Released/"&gt;WildFly 28.0.1 is released!&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2023/05/kogito-1-38-0-released.html"&gt;KOGITO 1.38.0 is released!&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_decaf"&gt;Decaf&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Enough java? Too much jitters? Wants to peek outside the coffee cup for a sec? The decaf’s section is here for you!&lt;/p&gt; &lt;p&gt;To stay on today’s theme, we wanted to mention that my guide on &lt;a href="https://quarkus.io/guides/ansible"&gt;Automate Quarkus deployment with Ansible&lt;/a&gt; has been added to the project’s website. I hope it helps Quarkus user who wants to automate their deployment! And note that there is also a &lt;a href="https://www.wildfly.org/news/2023/01/10/ansible-wildfly/"&gt;Ansible collection (extension) dedicated to Wildfly&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;A last note on Ansible, that may interest people of the community. There is a very cool new project called &lt;a href="https://www.ansible.com/blog/getting-started-with-event-driven-ansible"&gt;Event Driven Ansible&lt;/a&gt; using, behind the curtains, another cool project of the JBoss community: &lt;a href="https://www.drools.org/"&gt;Drools&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;&lt;em&gt;That’s all folks! Please join us again in two weeks for another round of our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/romain-pelisse 2023-06-01.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Romain Pelisse 2023-06-01&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Romain Pelisse 2023-06-01</dc:creator></entry><entry><title>Improvements to static analysis in the GCC 13 compiler</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/31/improvements-static-analysis-gcc-13-compiler" /><author><name>David Malcolm</name></author><id>9fcab65e-5911-46ef-862f-4e4d1c3b4504</id><updated>2023-05-31T07:00:00Z</updated><published>2023-05-31T07:00:00Z</published><summary type="html">&lt;p&gt;I work at Red Hat on &lt;a href="https://gcc.gnu.org/"&gt;GCC, the GNU Compiler Collection&lt;/a&gt;. For the last four releases of GCC, I've been working on &lt;code&gt;-fanalyzer&lt;/code&gt;, a static analysis pass that tries to identify various problems at compile-time, rather than at runtime. It performs "symbolic execution" of &lt;a href="https://developers.redhat.com/topics/c"&gt;C&lt;/a&gt; source code—effectively simulating the behavior of the code along the various possible paths of execution through it (with some caveats that we'll discuss).&lt;/p&gt; &lt;p&gt;This article summarizes what's new with &lt;code&gt;-fanalyzer&lt;/code&gt; in &lt;a href="https://gcc.gnu.org/gcc-13/changes.html"&gt;GCC 13&lt;/a&gt;, which has just been released.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;[ Learn more: &lt;a href="https://developers.redhat.com/articles/2023/05/04/new-c-features-gcc-13" target="_blank"&gt;New C features in GCC 13&lt;/a&gt; ] &lt;/strong&gt;&lt;/p&gt; &lt;h2&gt;New warnings&lt;/h2&gt; &lt;p&gt;I first added the analyzer to GCC in &lt;a href="https://developers.redhat.com/blog/2020/03/26/static-analysis-in-gcc-10/"&gt;GCC 10&lt;/a&gt;, with 15 new warnings for the compiler, and we've added more in each subsequent release (Table 1).&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="500"&gt;&lt;caption&gt; &lt;p class="text-align-left"&gt;Table 1: GCC warnings controlled by &lt;code&gt;-fanalyzer&lt;/code&gt; by release&lt;/p&gt; &lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th class="text-align-center" scope="row"&gt;Release&lt;/th&gt; &lt;th class="text-align-center" scope="col"&gt;New warnings&lt;/th&gt; &lt;th class="text-align-center" scope="col"&gt;Cumulative warnings&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th class="text-align-center" scope="row"&gt;&lt;a href="https://developers.redhat.com/blog/2020/03/26/static-analysis-in-gcc-10"&gt;GCC 10&lt;/a&gt;&lt;/th&gt; &lt;td class="text-align-center"&gt;15&lt;/td&gt; &lt;td class="text-align-center"&gt;15&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;th class="text-align-center" scope="row"&gt;&lt;a href="https://developers.redhat.com/blog/2021/01/28/static-analysis-updates-in-gcc-11"&gt;GCC 11&lt;/a&gt;&lt;/th&gt; &lt;td class="text-align-center"&gt;7&lt;/td&gt; &lt;td class="text-align-center"&gt;22&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;th class="text-align-center" scope="row"&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/12/state-static-analysis-gcc-12-compiler#"&gt;GCC 12&lt;/a&gt;&lt;/th&gt; &lt;td class="text-align-center"&gt;5&lt;/td&gt; &lt;td class="text-align-center"&gt;27&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;th class="text-align-center" scope="row"&gt;&lt;a href="https://gcc.gnu.org/gcc-13/changes.html"&gt;GCC 13&lt;/a&gt;&lt;/th&gt; &lt;td class="text-align-center"&gt;20&lt;/td&gt; &lt;td class="text-align-center"&gt;47&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;As you can see in Table 1, GCC 13 is a big release for &lt;code&gt;-fanalyzer&lt;/code&gt;, adding 20 new warnings. Let's take a look at some of them.&lt;/p&gt; &lt;h3&gt;Track dynamic buffer size&lt;/h3&gt; &lt;p&gt;Can you spot the bug in the following C code?&lt;/p&gt; &lt;pre&gt; &lt;code class="language-c"&gt;#include &lt;stdlib.h&gt; #include &lt;string.h&gt; struct str { size_t len; char data[]; }; struct str * make_str_badly (const char *src) { size_t len = strlen(src); struct str *str = malloc(sizeof(str) + len); if (!str) return NULL; str-&gt;len = len; memcpy(str-&gt;data, src, len); str-&gt;data[len] = '\0'; return str; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;span&gt;The above example &lt;/span&gt;makes the common mistake with C-style strings of forgetting the null terminator when computing how much space to allocate for &lt;code&gt;str&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;GCC 13's &lt;code&gt;-fanalyzer&lt;/code&gt; option now keeps track of the sizes of dynamically allocated buffers, and for many cases it checks the simulated memory reads and writes against the sizes of the relevant buffers. With this new work it detects the above problem by &lt;a href="https://godbolt.org/z/Y3v3c35zY"&gt;emitting this new warning&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;&lt;source&gt;: In function 'make_str_badly': &lt;source&gt;:18:18: warning: heap-based buffer overflow [CWE-122] [-Wanalyzer-out-of-bounds] 18 | str-&gt;data[len] = '\0'; | ~~~~~~~~~~~~~~~^~~~~~ 'make_str_badly': events 1-4 | | 13 | struct str *str = malloc(sizeof(str) + len); | | ^~~~~~~~~~~~~~~~~~~~~~~~~ | | | | | (1) capacity: 'len + 8' bytes | 14 | if (!str) | | ~ | | | | | (2) following 'false' branch (when 'str' is non-NULL)... | 15 | return NULL; | 16 | str-&gt;len = len; | | ~~~~~~~~~~~~~~ | | | | | (3) ...to here | 17 | memcpy(str-&gt;data, src, len); | 18 | str-&gt;data[len] = '\0'; | | ~~~~~~~~~~~~~~~~~~~~~ | | | | | (4) write of 1 byte at offset 'len + 8' exceeds the buffer |&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I want to thank &lt;a href="https://tim-lange.me/gsoc/"&gt;Tim Lange&lt;/a&gt; who implemented this warning as part of Google's Summer of Code program last year (along with two other new warnings: &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-allocation-size"&gt;&lt;code&gt;-Wanalyzer-allocation-size&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-imprecise-fp-arithmetic"&gt;&lt;code&gt;-Wanalyzer-imprecise-fp-arithmetic&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt; &lt;h3&gt;Check if NULL is dereferenced&lt;/h3&gt; &lt;p&gt;Here's an example of another new warning—what's wrong with the following C code?&lt;/p&gt; &lt;pre&gt; &lt;code class="language-c"&gt;#include &lt;assert.h&gt; #include &lt;stdio.h&gt; extern FILE *logfile; struct obj { const char *name; int x; int y; }; int is_within_boundary (struct obj *p, int radius_squared) { fprintf (logfile, "%s: (%i, %i)\n", p-&gt;name, p-&gt;x, p-&gt;y); if (!p) return 0; return (p-&gt;x * p-&gt;x) + (p-&gt;y * p-&gt;y) &lt; radius_squared; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The issue is that the code is unclear about whether &lt;code&gt;p&lt;/code&gt; can be &lt;code&gt;NULL&lt;/code&gt;: it's dereferenced unconditionally at the &lt;code&gt;fprintf&lt;/code&gt; call, but then checked for &lt;code&gt;NULL&lt;/code&gt; later on. A pointer that's unconditionally dereferenced can be assumed by a compiler to be non-&lt;code&gt;NULL&lt;/code&gt;, and thus the check against &lt;code&gt;NULL&lt;/code&gt; can potentially be optimized away, which is probably not want you want—but the compiler has no way to know what you meant.&lt;/p&gt; &lt;p&gt;As of GCC 13, the &lt;code&gt;-fanalyzer&lt;/code&gt; option now detects the above by &lt;a href="https://godbolt.org/z/7G35YxoP3"&gt;emitting this warning&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;&lt;source&gt;: In function 'is_within_boundary': &lt;source&gt;:16:6: warning: check of 'p' for NULL after already dereferencing it [-Wanalyzer-deref-before-check] 16 | if (!p) | ^ 'is_within_boundary': events 1-2 | | 15 | fprintf (logfile, "%s: (%i, %i)\n", p-&gt;name, p-&gt;x, p-&gt;y); | | ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ | | | | | (1) pointer 'p' is dereferenced here | 16 | if (!p) | | ~ | | | | | (2) pointer 'p' is checked for NULL here but it was already dereferenced at (1) | &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Other new warnings&lt;/h3&gt; &lt;p&gt;I don't have space in this article to give examples of every new warning added in GCC 13, but here's a round-up of the others.&lt;/p&gt; &lt;p&gt;I added support to &lt;code&gt;-fanalyzer&lt;/code&gt; for tracking the state of &lt;code&gt;&lt;stdarg.h&gt;&lt;/code&gt;:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-va-list-leak"&gt;&lt;code&gt;-Wanalyzer-va-list-leak&lt;/code&gt;&lt;/a&gt; for complaining about missing &lt;code&gt;va_end&lt;/code&gt; after a &lt;code&gt;va_start&lt;/code&gt; or &lt;code&gt;va_copy&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-va-list-use-after-va-end"&gt;&lt;code&gt;-Wanalyzer-va-list-use-after-va-end&lt;/code&gt;&lt;/a&gt; for complaining about &lt;code&gt;va_arg&lt;/code&gt; or &lt;code&gt;va_copy&lt;/code&gt; used on a &lt;code&gt;va_list&lt;/code&gt; that's had &lt;code&gt;va_end&lt;/code&gt; called on it&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-va-arg-type-mismatch"&gt;&lt;code&gt;-Wanalyzer-va-arg-type-mismatch&lt;/code&gt;&lt;/a&gt; for type-checking of &lt;code&gt;va_arg&lt;/code&gt; usage in interprocedural execution paths against the types of the parameters that were actually passed to the variadic call&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-va-list-exhausted"&gt;&lt;code&gt;-Wanalyzer-va-list-exhausted&lt;/code&gt;&lt;/a&gt; for complaining in interprocedural execution paths if &lt;code&gt;va_arg&lt;/code&gt; is used too many times on a &lt;code&gt;va_list&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;span dir="ltr"&gt;&lt;a href="https://gist.github.com/mirimmad/9524fa6ada8dda5436447dcc4cfc86f0"&gt;Immad Mir&lt;/a&gt; implemented tracking of file descriptors within the analyzer as part of Google Summer of Code 2022. We added seven new warnings relating to this in GCC 13:&lt;/span&gt;&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-fd-access-mode-mismatch"&gt;&lt;code&gt;-Wanalyzer-fd-access-mode-mismatch&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-fd-double-close"&gt;&lt;code&gt;-Wanalyzer-fd-double-close&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-fd-leak"&gt;&lt;code&gt;-Wanalyzer-fd-leak&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-fd-phase-mismatch"&gt;&lt;code&gt;-Wanalyzer-fd-phase-mismatch&lt;/code&gt;&lt;/a&gt; (e.g. calling &lt;code&gt;accept&lt;/code&gt; on a socket before calling &lt;code&gt;listen&lt;/code&gt; on it)&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-fd-type-mismatch"&gt;&lt;code&gt;-Wanalyzer-fd-type-mismatch&lt;/code&gt;&lt;/a&gt; (e.g. using a stream socket operation on a datagram socket)&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-fd-use-after-close"&gt;&lt;code&gt;-Wanalyzer-fd-use-after-close&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-fd-use-without-check"&gt;&lt;code&gt;-Wanalyzer-fd-use-without-check&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;along with &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Common-Function-Attributes.html#index-fd_005farg-function-attribute"&gt;attributes for marking &lt;code&gt;int&lt;/code&gt; function arguments as being file descriptors&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Finally, I implemented various other warnings:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-exposure-through-uninit-copy"&gt;&lt;code&gt;-Wanalyzer-exposure-through-uninit-copy&lt;/code&gt;&lt;/a&gt; (for detecting "infoleaks" in the Linux kernel)&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-infinite-recursion"&gt;&lt;code&gt;-Wanalyzer-infinite-recursion&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-jump-through-null"&gt;&lt;code&gt;-Wanalyzer-jump-through-null&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-putenv-of-auto-var"&gt;&lt;code&gt;-Wanalyzer-putenv-of-auto-var&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Static-Analyzer-Options.html#index-Wanalyzer-tainted-assertion"&gt;&lt;code&gt;-Wanalyzer-tainted-assertion&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;SARIF output&lt;/h2&gt; &lt;p&gt;In GCC 9 I added an option &lt;code&gt;-fdiagnostics-format=json&lt;/code&gt; &lt;span dir="ltr"&gt;to provide &lt;a href="https://developers.redhat.com/blog/2019/03/08/usability-improvements-in-gcc-9#not_just_for_humans"&gt;machine-readable output for GCC's diagnostics&lt;/a&gt;. This is a custom JSON-based format that closely follows GCC's own internal representation.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;In the meantime, another JSON-based format has emerged as the standard in this space: &lt;a href="https://sarifweb.azurewebsites.net/"&gt;SARIF (the Static Analysis Results Interchange Format)&lt;/a&gt;. This file format is suited for capturing the results of static analysis tools (like GCC's &lt;code&gt;-fanalyzer&lt;/code&gt;), but it can also be used for plain GCC warnings and errors.&lt;/p&gt; &lt;p&gt;So for GCC 13 I've extended &lt;code&gt;-fdiagnostics-format=&lt;/code&gt; to add two new options implementing SARIF support: &lt;code&gt;-fdiagnostics-format=sarif-stderr&lt;/code&gt; and &lt;code&gt;-fdiagnostics-format=sarif-file&lt;/code&gt;. I've also joined the &lt;a href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=sarif"&gt;technical committee&lt;/a&gt; overseeing the standard.&lt;/p&gt; &lt;p&gt;By producing data in an industry standard format we benefit from interoperability with existing consumers of SARIF data. Figure 1 is a simple example, showing VS Code (with a SARIF plugin) viewing a SARIF file generated by GCC. The IDE is able to annotate the source code, adding squiggly lines under code where GCC finds problems. Here I've clicked on a line where &lt;code&gt;-fanalyzer&lt;/code&gt; reported a double-free bug, and the IDE is showing the path of execution through the code that GCC predicted will trigger the problem.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/2022-06-06-vscode-showing-gcc-sarif-output.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/2022-06-06-vscode-showing-gcc-sarif-output.png?itok=Rg5L9WZ5" width="881" height="689" alt="Screenshot of VS Code showing GCC SARIF output" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: GCC SARIF output in VS Code.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Fixing false positives&lt;/h2&gt; &lt;p&gt;Static analyzers are not perfect—it's impossible to reason perfectly about the most interesting properties of source code. The GCC analyzer performs a crude simulation of the state of the inside of the program, and I've made many tradeoffs to try to make it fast enough to use when working on code. I receive anecdotal reports that people are using it and it's finding bugs for them earlier than they would have found them otherwise, but there will be false positives and false negatives. The analyzer is a bug-finding tool, rather than a tool for proving program correctness (and, alas, sometimes bugs lead to it being too slow). In technical terms, it's neither "sound" nor "complete." &lt;/p&gt; &lt;p&gt;I've spent the first few months of this year trying to reduce "spam" from the analyzer for GCC 13. I created an &lt;a href="https://github.com/davidmalcolm/gcc-analyzer-integration-tests"&gt;integration testing suite&lt;/a&gt;: I picked various real-world C projects, including Doom, the &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; kernel, and qemu. I've been building them with their standard options, but with &lt;code&gt;-fanalyzer&lt;/code&gt; added to the build flags, examining the warnings emitted, and trying to fix the false positives.&lt;/p&gt; &lt;p&gt;I made a lot of fixes to the analyzer; Table 2 shows some before and after numbers for the warnings that were most improved by this work, where FP means a "false positive" (a bogus warning about a non-problem) and TP means a "true positive" (a valid warning about a real problem in the source code).&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="645"&gt;&lt;caption&gt; &lt;p class="text-align-left"&gt;Table 2: Improved warnings.&lt;/p&gt; &lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th class="text-align-center"&gt;Warning&lt;/th&gt; &lt;th class="text-align-center"&gt; &lt;p&gt;FP&lt;/p&gt; &lt;p&gt;before&lt;/p&gt; &lt;/th&gt; &lt;th class="text-align-center"&gt; &lt;p&gt;FP&lt;/p&gt; &lt;p&gt;after&lt;/p&gt; &lt;/th&gt; &lt;th class="text-align-center"&gt; &lt;p&gt;TP&lt;/p&gt; &lt;p&gt;before&lt;/p&gt; &lt;/th&gt; &lt;th class="text-align-center"&gt; &lt;p&gt;TP&lt;/p&gt; &lt;p&gt;after&lt;/p&gt; &lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;-Wanalyzer-deref-before-check&lt;/code&gt;&lt;/td&gt; &lt;td class="text-align-right"&gt;63&lt;/td&gt; &lt;td class="text-align-right"&gt;12&lt;/td&gt; &lt;td class="text-align-right"&gt;1&lt;/td&gt; &lt;td class="text-align-right"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;-Wanalyzer-malloc-leak&lt;/code&gt;&lt;/td&gt; &lt;td class="text-align-right"&gt;78&lt;/td&gt; &lt;td class="text-align-right"&gt;50&lt;/td&gt; &lt;td class="text-align-right"&gt;0&lt;/td&gt; &lt;td class="text-align-right"&gt;61&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;-Wanalyzer-use-of-uninitialized-value&lt;/code&gt;&lt;/td&gt; &lt;td class="text-align-right"&gt;998&lt;/td&gt; &lt;td class="text-align-right"&gt;125&lt;/td&gt; &lt;td class="text-align-right"&gt;0&lt;/td&gt; &lt;td class="text-align-right"&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;You can see that I eliminated most (but not all) of the false positives from &lt;code&gt;-Wanalyzer-deref-before-check&lt;/code&gt; , and that I reduced the number of FPs from &lt;code&gt;-Wanalyzer-malloc-leak&lt;/code&gt; whilst fixing it so that it correctly detected a bunch of real memory leaks that it had previously missed (in Doom's initialization logic, as it happens). Unfortunately, &lt;code&gt;-Wanalyzer-use-of-uninitialized-value&lt;/code&gt; is still the "spammiest" warning, despite me making a big dent in its number of FPs; it seems to be most prone to exploring paths through the code that can't happen in practice, where the analyzer doesn't have enough high-level information about invariants in the code to figure that out.&lt;/p&gt; &lt;h2&gt;Trying it out&lt;/h2&gt; &lt;p&gt;GCC 13 has been released upstream, and is the system compiler in the &lt;a href="https://fedoramagazine.org/announcing-fedora-38/"&gt;recently-released Fedora 38&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For simple C examples, you can play around with the new GCC online at the &lt;a href="https://godbolt.org/"&gt;Compiler Explorer site&lt;/a&gt;. Select GCC 13.1 and add &lt;code&gt;-fanalyzer&lt;/code&gt; to the compiler options to run static analysis.&lt;/p&gt; &lt;p&gt;As noted above, the analyzer isn't perfect, but I hope it's helpful. Given that every compiler and analyzer finds a slightly different subset of bugs it's usually a good idea to run your code through more than one toolchain to see what shakes out.&lt;/p&gt; &lt;p&gt;Finally, if you're interested in getting involved in compiler development, I've written a &lt;a href="https://gcc-newbies-guide.readthedocs.io/en/latest/"&gt;guide to getting started as a GCC contributor&lt;/a&gt;. It includes lots of ideas for new warnings and features in GCC's Bugzilla.&lt;/p&gt; &lt;p&gt;Have fun!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/31/improvements-static-analysis-gcc-13-compiler" title="Improvements to static analysis in the GCC 13 compiler"&gt;Improvements to static analysis in the GCC 13 compiler&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>David Malcolm</dc:creator><dc:date>2023-05-31T07:00:00Z</dc:date></entry><entry><title type="html">KeyCloak Social Login Step-by-Step guide</title><link rel="alternate" href="https://www.mastertheboss.com/keycloak/google-social-login-with-keycloak/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/keycloak/google-social-login-with-keycloak/</id><updated>2023-05-31T06:25:35Z</updated><content type="html">Are you looking to enhance your application’s user authentication process? Configuring social login in Keycloak can provide a seamless and convenient login experience for your users. In this comprehensive tutorial, we will guide you through the step-by-step process of setting up social login in Keycloak, leveraging Google Identity Provider as an example. So let’s get ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Quarkus 3.1.0.Final released - Programmatic creation of Reactive REST Clients, Kotlin 1.8.21 and more</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-3-1-0-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-3-1-0-final-released/</id><updated>2023-05-31T00:00:00Z</updated><content type="html">It has been a month since we released Quarkus 3.0 and it is our pleasure to announce Quarkus 3.1.0.Final. As usual, it comes with a lot of improvements all over the place. Major changes are: Provide new API to programmatically create Reactive REST Clients Introduce a way to set headers...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title>Why use RHEL for SAP Solutions?</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/30/why-use-rhel-sap-solutions" /><author><name>Nikhil Mungale</name></author><id>8ef5986f-2a99-48a5-afe1-33e4654381e5</id><updated>2023-05-30T13:30:00Z</updated><published>2023-05-30T13:30:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/linux/"&gt;Red Hat Enterprise Linux&lt;/a&gt; &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;(RHEL) for SAP Solutions is a secure and scalable platform tailored to the needs of SAP workloads, such as SAP NetWeaver, SAP S/4HANA, and the SAP HANA platform. As SAP Business Suite will be out of standard support by 2027, it is essential for developers and administrators to plan for a smooth migration to the SAP S/4 HANA platform. &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;In this article, we'll explore what RHEL for SAP Solutions is and the features and functionalities it provides to developers and administrators.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;What is Red Hat Enterprise Linux for SAP Solutions?&lt;/h2&gt; &lt;p&gt;Red Hat Enterprise Linux for SAP Solutions is tailored for SAP workloads such as the SAP HANA platform and S/4HANA. The SAP environment can be standardized on Red Hat Enterprise Linux. &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;By standardizing the SAP environment on RHEL, businesses can streamline operations, reduce costs, and benefit from integrated smart management and high-availability solutions included in the offering&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. &lt;/p&gt; &lt;h2&gt;What's included in the RHEL for SAP Solutions subscription?&lt;/h2&gt; &lt;p&gt;Built on the foundation of Red Hat Enterprise Linux (RHEL) for SAP Applications along with its packages and components,  a RHEL for SAP Solutions subscription includes the following:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;SAP-specific technical components to support SAP S/4HANA, SAP HANA, and SAP Business Applications.&lt;/li&gt; &lt;li aria-level="1"&gt;SAP-specific high availability solutions for SAP S/4HANA, SAP HANA, and SAP Business Applications.&lt;/li&gt; &lt;li aria-level="1"&gt;RHEL System Roles for SAP for automation of operating system configuration to run SAP workloads.&lt;/li&gt; &lt;li aria-level="1"&gt;SAP-tailored Red Hat Insights Dashboard and Smart Management help streamline operations and reduce costs&lt;/li&gt; &lt;li aria-level="1"&gt;Update Services for SAP Solutions or Extended Update Support (EUS), providing support for specific minor RHEL releases for up to four years from General Availability. See the &lt;a href="https://access.redhat.com/support/policy/updates/errata/"&gt;RHEL Life Cycle&lt;/a&gt; web page for more information about EUS and Update Services for SAP Solutions.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;All the packages provided by RHEL for SAP Applications are also provided by RHEL for SAP Solutions. &lt;/p&gt; &lt;p&gt;Learn more about the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_sap_solutions/9/html/overview_of_red_hat_enterprise_linux_for_sap_solutions_subscription/index"&gt;RHEL for SAP solutions subscription&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;What are the benefits of RHEL for SAP Solutions?&lt;/h2&gt; &lt;p&gt;Because the SAP Business Suite is going out of standard support by 2027, existing applications on SAP Business Suite need to migrate to S/4HANA, which will be available only on x86/Power and &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt;. RHEL provides migration support for such cases, ensuring a smooth and hassle-free transition. This benefit for businesses is that they can seamlessly migrate their SAP solutions to RHEL without disrupting business operations. &lt;/p&gt; &lt;p&gt;With Red Hat Insights included in the subscription, RHEL for SAP Solutions provides technologies that prioritize recommendations based on deep analysis with targeted expertise. This allows organizations to evaluate all of their ecosystems, including hybrid cloud environments, at no extra cost.&lt;/p&gt; &lt;p&gt;RHEL also provides powerful analytical tools for monitoring SAP applications and solutions, ensuring optimal performance and stability. With these tools, administrators can quickly identify and troubleshoot issues, reducing the risk of downtime and improving the system's overall performance. &lt;a href="https://developers.redhat.com/products/rhel/download#rhel3ways"&gt;Download RHEL for SAP solutions packages&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Migrate to SAP S/4 HANA and SAP HANA with RHEL&lt;/h2&gt; &lt;p&gt;Migration to SAP S/4 HANA can be complex and costly. Administrators must deal with outdated, legacy hardware, and cases where migration may be challenging. They need to have a flexible and secure platform that provides evolving software technologies and has the ability to port applications seamlessly.  &lt;/p&gt; &lt;p&gt;Red Hat solutions for SAP workloads unify administration and management, &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;enabling SAP customers to avoid using disparate toolsets from different vendors to manage their virtual infrastructure, high availability cluster, and operating system&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Red Hat’s solution for lifecycle management can be used to operate all infrastructure layers and technologies with a single user experience. &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_sap_solutions/9/html/how_to_in-place_upgrade_sap_environments_from_rhel_8_to_rhel_9/index"&gt;Learn how to subscribe to Update Services for SAP Solutions on RHEL 8 and RHEL 9.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;RHEL for SAP Solutions includes high availability and proactive monitoring capabilities, remote management options, and extended support subscriptions, so developers and administrators can focus on achieving business objectives.&lt;/p&gt; &lt;h2&gt;Run and manage SAP applications on hyperscalers with RHEL&lt;/h2&gt; &lt;p&gt;SAP customers are looking for the ideal foundation for the hybrid cloud that allows for native application development and modernization to keep the digital core clean for deploying and extending SAP into a true hybrid cloud. &lt;/p&gt; &lt;p&gt;Running Red Hat Enterprise Linux for SAP Solutions on the public cloud can provide a single agnostic and consistent set of infrastructure tooling that can be used on-prem and on any cloud. Organizations can choose the cloud provider best for their workloads and be free to move those workloads as needed. Red Hat Enterprise Linux is certified on all the major cloud providers and has an extensive ecosystem of certified hardware platforms, value-add software integrations, and ISV software.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In summary, running RHEL for SAP Solutions provides enterprises with a reliable, secure, high-performance platform optimized for running critical business systems. With Red Hat Enterprise Linux's migration support, high availability capabilities, live kernel patching, and powerful analytical tools, businesses can ensure that their SAP applications and solutions are always available, reliable, and efficient. &lt;a href="https://developers.redhat.com/products/rhel/download#rhel3ways"&gt;Download RHEL for SAP Solutions and Applications packages&lt;/a&gt; to get started.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/30/why-use-rhel-sap-solutions" title="Why use RHEL for SAP Solutions?"&gt;Why use RHEL for SAP Solutions?&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Nikhil Mungale</dc:creator><dc:date>2023-05-30T13:30:00Z</dc:date></entry><entry><title>Build an all-in-one edge manager with single-node OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/30/build-edge-manager-single-node-openshift" /><author><name>Benjamin Schmaus, Josh Swanson</name></author><id>8727de2f-875d-4381-ae3d-638c2c1a1da1</id><updated>2023-05-30T07:00:00Z</updated><published>2023-05-30T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; includes an abundance of technologies out of the box that are necessary for effectively managing a fleet of devices at the &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;edge&lt;/a&gt;. One of those components, the scheduler, enables these services to be efficiently co-located onto a single platform.&lt;/p&gt; &lt;p&gt;In addition, OpenShift manages many of these services via an &lt;a href="https://developers.redhat.com/topics/kubernetes/operators/"&gt;Operator&lt;/a&gt;, meaning a non-technical team doesn’t need to understand all the specific details about the service. OpenShift, in a sense, helps make managing devices at the edge simpler and cost-effective.&lt;/p&gt; &lt;p&gt;This article details how to go about building this configuration on a single-node OpenShift cluster. Keep in mind that you can apply these same concepts to a 3-node compact and full OpenShift cluster as well.&lt;/p&gt; &lt;h2&gt;Why use OpenShift on a single node?&lt;/h2&gt; &lt;p&gt;Before we begin, let us step back and ask: Why do this? Well, there are a variety of reasons:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Having all the components in a single-node OpenShift (SNO) cluster makes it a great way to have a one-stop experience.&lt;/li&gt; &lt;li&gt;Having all the components in a single OpenShift node provides a quick and easy way to prove out a concept.&lt;/li&gt; &lt;li&gt;Since it's OpenShift, the SNO concept can be graduated to a large cluster to meet the capacity needs of a production environment.&lt;/li&gt; &lt;li&gt;Device Edge images are really just YAML files that should be maintained in Git, which gives us a clear path to infrastructure as code (IaC) and proper continuous integration/continuous deployment (&lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;CI/CD&lt;/a&gt;) all within OpenShift Container Platform.&lt;/li&gt; &lt;li&gt;Operators are genuinely a great way to reduce the barrier of entry when it comes to installing services and components in OpenShift.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Components&lt;/h2&gt; &lt;p&gt;Now that we understand some of the why, let's move forward and lay out what components we will be using in this single-node OpenShift "edge manager in a box." The core set of services we’ll be consuming are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;An image registry to store our edge images as we compose them.&lt;/li&gt; &lt;li&gt;Management of local storage for retaining our composed images, databases, etc.&lt;/li&gt; &lt;li&gt;An instance of Ansible automation controller to drive our automation and leverage existing automation.&lt;/li&gt; &lt;li&gt;A pipeline technology; we’ll be using Red Hat OpenShift Pipelines.&lt;/li&gt; &lt;li&gt;A virtualization platform such as Red Hat OpenShift Virtualization (formerly container-native virtualization).&lt;/li&gt; &lt;li&gt;A virtual machine template to deploy virtual machines from which we can build our images.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;These core services, when integrated together, offer the functionality necessary for managing our fleet of device edge devices.&lt;/p&gt; &lt;p&gt;There are different ways to deploy workloads on OpenShift. However, because we’ll be consuming a handful of Operators, we find it's useful to leverage &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; to get everything deployed. Ansible has a module in the kubernetes.core collection (k8s) that can be leveraged to talk directly to the Kubernetes API. We’ll use it here to push k8s objects related to installing Operators and creating instances from those Operators.&lt;/p&gt; &lt;h2&gt;The wrapper playbook&lt;/h2&gt; &lt;p&gt;The first playbook we need to create on our quest for edge device management is a wrapper playbook that will ultimately call all the playbooks to build out our environment. The playbook will look like the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: import playbook to configure the local registry ansible.builtin.import_playbook: configure-registry.yml - name: import playbook to setup local storage ansible.builtin.import_playbook: configure-storage.yml - name: import playbook to setup controller ansible.builtin.import_playbook: install-ansible.yml - name: import playbook to setup pipelines ansible.builtin.import_playbook: configure-pipelines.yml - name: import playbook to setup virtualization ansible.builtin.import_playbook: configure-virtualization.yml - name: import playbook to setup image builder virtual machine template ansible.builtin.import_playbook: setup-image-builder-vm-template.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This playbook simply imports other playbooks that contain the actual steps necessary to get an Operator installed: create an OperatorGroup, deploy an instance, and more. We won’t go through all of these playbooks, but let’s take a deep dive on the playbook to set up &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;:&lt;/p&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: install controller hosts: - sno_clusters gather_facts: false module_defaults: kubernetes.core.k8s: kubeconfig: "{{ tmpdir.path }}/ocp/auth/kubeconfig" tasks: - name: configure storage delegate_to: localhost block: - name: create namespace kubernetes.core.k8s: definition: "{{ lookup('file', 'files/namespaces/ansible-automation-platform.yaml') | from_yaml }}" - name: create operator group kubernetes.core.k8s: definition: "{{ lookup('file', 'files/operator-groups/ansible-automation-platform.yaml') | from_yaml }}" - name: install operator kubernetes.core.k8s: definition: "{{ lookup('file', 'files/operators/ansible-automation-platform.yaml') | from_yaml }}" register: operator_install until: - operator_install.result.status.state is defined - operator_install.result.status.state == 'AtLatestKnown' retries: 100 delay: 10 - name: create instance of controller kubernetes.core.k8s: definition: "{{ lookup('file', 'files/instances/controller.yaml') | from_yaml }}" &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;p&gt;The playbook above is grabbing files that contain k8s objects and pushing them into the Kubernetes API. For Ansible Automation Platform specifically, we have a namespace, an Operator group, a subscription, and then an instance of Controller. First, the namespace custom resource YAML:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: v1 kind: Namespace metadata: labels: openshift.io/cluster-monitoring: "true" name: ansible-automation-platform&lt;/code&gt;&lt;/pre&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;p&gt;Next, we have the Operator group custom resource YAML:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: ansible-automation-platform-operator namespace: ansible-automation-platform spec: targetNamespaces: - ansible-automation-platform&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;p&gt;Then comes the subscription custom resource YAML:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: ansible-automation-platform namespace: ansible-automation-platform spec: channel: 'stable-2.3' installPlanApproval: Automatic name: ansible-automation-platform-operator source: redhat-operators sourceNamespace: openshift-marketplace&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, once the Operator finishes deploying, an instance of the controller custom resource YAML:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: automationcontroller.ansible.com/v1beta1 kind: AutomationController metadata: name: controller namespace: ansible-automation-platform spec: replicas: 1&lt;/code&gt;&lt;/pre&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;p&gt;After a few minutes, we’ll have a running instance of the Ansible automation controller on our OpenShift cluster.&lt;/p&gt; &lt;/div&gt; &lt;h2&gt;Configuring Ansible automation controller&lt;/h2&gt; &lt;p&gt;If we already have an instance of Controller set up and configured, then this part isn’t necessary. However, if we're starting from a completely empty instance of Controller, then we need to apply some base configuration to it so it can start driving automation.&lt;/p&gt; &lt;p&gt;Note: A best practice with automation controller is to store the configuration in code, then leverage automation to deploy the configuration to Controller. Here, we’ll leverage the redhat_cop.controller_configuration collection.  First, we’ll need some specific credential types:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;OpenShift kubeconfig:&lt;/strong&gt; A credential type to inject a kubeconfig into the execution environment of our automation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Red Hat Subscription Management credentials:&lt;/strong&gt; A credential type for storing authentication details for Red Hat Customer Portal.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Image credentials:&lt;/strong&gt; A credential type for securely storing the user account credentials we want in our composed images, as opposed to storing these in plain text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ansible controller API credentials:&lt;/strong&gt; A set of credentials to authenticate to automation controller’s API.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kube credentials:&lt;/strong&gt; A set of credentials that will be used to authenticate to our registry. I’m using OpenShift’s internal registry and the kubeadmin account, but you can substitute a properly scoped account and use a registry of your choosing.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The YAML definitions of these custom credential types:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;controller_credential_types: - name: Openshift Kubeconfig kind: cloud inputs: fields: - id: kubeconfig type: string label: Kubeconfig #secret: true multiline: true injectors: env: K8S_AUTH_KUBECONFIG: "{ { tower.filename.kubeconfig }}" KUBECONFIG: "{ { tower.filename.kubeconfig }}" file: template.kubeconfig: "{ { kubeconfig }}" - name: RHSM Credentials kind: cloud inputs: fields: - id: rhsm_username type: string label: RHSM Hostname - id: rhsm_password type: string label: RHSM Username secret: true injectors: extra_vars: rhsm_username: "{ { rhsm_username }}" rhsm_password: "{ { rhsm_password }}" - name: Image Credentials kind: cloud inputs: fields: - id: image_username type: string label: Image Hostname - id: image_password type: string label: Image Username secret: true injectors: extra_vars: image_username: "{ { image_username }}" image_password: "{ { image_password }}" - name: Ansible Controller API Credentials kind: cloud inputs: fields: - id: controller_hostname type: string label: Controller Hostname - id: controller_username type: string label: Controller Username - id: controller_password type: string label: Controller Password secret: yes injectors: extra_vars: controller_hostname: "{ { controller_hostname }}" controller_username: "{ { controller_username }}" controller_password: "{ { controller_password }}" controller_validate_certs: "no" - name: Kubeadmin Credentials kind: cloud inputs: fields: - id: kubeadmin_username type: string label: Kubeadmin username - id: kubeadmin_password type: string label: Kubeadmin password secret: true injectors: extra_vars: kubeadmin_username: "{ { kubeadmin_username }}" kubeadmin_password: "{ { kubeadmin_password }}"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Most of these credential types are straightforward; however, the kubeconfig credential type has some additional injectors in the form of a file and an environment variable of the path to that file. In addition, the leading two brackets in the injector configurations are how we tell the collection to send an “unsafe” string to the API without attempting to render it locally. Leveraging our new credential types, we can create the set of credentials we’ll need for our automation:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;controller_credentials: - name: kubeconfig organization: Default credential_type: Openshift Kubeconfig inputs: kubeconfig: "{{ lookup('file', (tmpdir.path + '/ocp/auth/kubeconfig')) | from_yaml | string }}" - name: Machine Credentials organization: Default credential_type: Machine inputs: username: cloud-user password: "{{ vm_template_password }}" become_password: "{{ vm_template_password }}" - name: Ansible Controller API Credentials credential_type: Ansible Controller API Credentials organization: Default inputs: controller_hostname: "{{ controller_hostname }}" controller_username: admin controller_password: "{{ controller_password }}" - name: RHSM Credentials credential_type: RHSM Credentials organization: Default inputs: rhsm_username: "{{ rhsm_username }}" rhsm_password: "{{ rhsm_password }}" - name: Image Credentials credential_type: Image Credentials organization: Default inputs: image_username: "{{ image_username }}" image_password: "{{ image_password }}" - name: Kubeadmin Credentials credential_type: Kubeadmin Credentials organization: Default inputs: kubeadmin_username: kubeadmin kubeadmin_password: "{{ lookup('file', (tmpdir.path + '/ocp/auth/kubeadmin-password')) }}"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, we’ll need an execution environment that contains the appropriate collections and &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; libraries. We’ll discuss the building of this execution environment later, but for now, this is the definition:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;controller_execution_environments: - name: Image Builder Execution Environment image: quay.io/device-edge-workshops/helper-ee:latest pull: always&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After our execution environment, we’ll set up two inventories: one scoped for performing “local actions,” where the execution node performs the work without needing to connect to a remote system, and another to contain our image builder system:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;controller_inventories: - name: Image Builder Servers organization: Default variables: k8s_api_address: "api.{{ inventory_hostname }}" k8s_api_int_address: "api-int.{{ inventory_hostname }}:6443" ocp_namespace: image-builder image_registry: 'image-registry.openshift-image-registry.svc.cluster.local:5000' - name: Local Actions organization: Default variables: k8s_api_address: "api.{{ inventory_hostname }}" k8s_api_int_address: "api-int.{{ inventory_hostname }}:6443" ocp_namespace: image-builder image_registry: 'image-registry.openshift-image-registry.svc.cluster.local:5000'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Be sure to define the inventory variables to correspond to your OpenShift cluster environment. Next, a simple host to use for local actions:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;controller_hosts: - name: localhost inventory: Local Actions variables: ansible_connection: local ansible_python_interpreter: "{ { ansible_playbook_python }}"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note: This has the same double spacing as above, meaning we’re sending a variable that will be resolved by Controller when it runs the automation, and not by the playbook configuring Controller right now. After that, a project containing our code:&lt;/p&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;controller_projects: - name: Image Builder Codebase organization: Default scm_type: git scm_url: https://github.com/redhat-manufacturing/device-edge-demos.git&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;p&gt;Finally, we define our job templates:&lt;/p&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;controller_templates: - name: Manage Virtual Machine Connectivity organization: Default inventory: Local Actions project: Image Builder Codebase playbook: demos/rhde-pipeline/playbooks/manage-vm-connection.yml execution_environment: Image Builder Execution Environment ask_variables_on_launch: true credentials: - kubeconfig - name: Manage Host in Controller organization: Default inventory: Local Actions project: Image Builder Codebase playbook: demos/rhde-pipeline/playbooks/manage-host-in-controller.yml execution_environment: Image Builder Execution Environment ask_variables_on_launch: true credentials: - kubeconfig - Ansible Controller API Credentials - name: Preconfigure Virtual Machine organization: Default inventory: Image Builder Servers project: Image Builder Codebase playbook: demos/rhde-pipeline/playbooks/preconfigure-virtual-machine.yml execution_environment: Image Builder Execution Environment ask_variables_on_launch: true become_enabled: true credentials: - Machine Credentials - RHSM Credentials - name: Install Image Builder organization: Default inventory: Image Builder Servers project: Image Builder Codebase playbook: demos/rhde-pipeline/playbooks/install-image-builder.yml execution_environment: Image Builder Execution Environment ask_variables_on_launch: true become_enabled: true credentials: - Machine Credentials - name: Manage Image Builder Connectivity organization: Default inventory: Local Actions project: Image Builder Codebase playbook: demos/rhde-pipeline/playbooks/manage-ib-connection.yml execution_environment: Image Builder Execution Environment ask_variables_on_launch: true credentials: - kubeconfig - name: Compose Image organization: Default inventory: Image Builder Servers project: Image Builder Codebase playbook: demos/rhde-pipeline/playbooks/compose-image.yml execution_environment: Image Builder Execution Environment ask_variables_on_launch: true become_enabled: true credentials: - Machine Credentials - Image Credentials - name: Push Image to Registry organization: Default inventory: Image Builder Servers project: Image Builder Codebase playbook: demos/rhde-pipeline/playbooks/push-image-to-registry.yml execution_environment: Image Builder Execution Environment ask_variables_on_launch: true become_enabled: true credentials: - Machine Credentials - Kubeadmin Credentials - name: Deploy Edge Container organization: Default inventory: Local Actions project: Image Builder Codebase playbook: demos/rhde-pipeline/playbooks/deploy-edge-container.yml execution_environment: Image Builder Execution Environment ask_variables_on_launch: true credentials: - kubeconfig&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;p&gt;A few things to note here: We’re consuming the credentials, inventories, project, and execution environment we created earlier. We’re also allowing some of these job templates to take additional variables when launched, a feature we’ll leverage later when building out our pipeline. Also, all of the referenced playbooks are available on GitHub as a starting point for building your own edge automation.&lt;/p&gt; &lt;h2&gt;Interfacing with automation controller&lt;/h2&gt; &lt;p&gt;Automation controller has a fully featured RESTful API that can be leveraged to perform basically every controller function, making it very easy to integrate with. However, we will do something a bit more custom, which will simplify our pipeline tasks and allow individual tasks to wait for the corresponding automation to complete.&lt;/p&gt; &lt;p&gt;A quick refresher: Execution environments are &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; images with roles, collections, Python libraries, and the Ansible bits pre-installed and ready to roll. Since we’re already operating within a container platform, we can reuse those execution environments within our pipeline tasks.&lt;/p&gt; &lt;p&gt;Because we’re building an execution environment, our collections and Python libraries will be included, meaning if we start the container, we can directly call Ansible. To extend the functionality a bit further, we’ll add a few steps to the build process and insert a playbook directly that we can leverage during our pipeline run.&lt;/p&gt; &lt;p&gt;Here’s an example Containerfile for our execution environment:&lt;/p&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;ARG EE_BASE_IMAGE=registry.redhat.io/ansible-automation-platform-23/ee-minimal-rhel8:latest ARG EE_BUILDER_IMAGE=registry.redhat.io/ansible-automation-platform-23/ansible-builder-rhel8 FROM $EE_BASE_IMAGE as galaxy ARG ANSIBLE_GALAXY_CLI_COLLECTION_OPTS= ARG ANSIBLE_GALAXY_CLI_ROLE_OPTS= USER root ADD _build /build WORKDIR /build RUN ansible-galaxy role install $ANSIBLE_GALAXY_CLI_ROLE_OPTS -r requirements.yml --roles-path "/usr/share/ansible/roles" RUN ANSIBLE_GALAXY_DISABLE_GPG_VERIFY=1 ansible-galaxy collection install $ANSIBLE_GALAXY_CLI_COLLECTION_OPTS -r requirements.yml --collections-path "/usr/share/ansible/collections" FROM $EE_BUILDER_IMAGE as builder COPY --from=galaxy /usr/share/ansible /usr/share/ansible ADD _build/requirements.txt requirements.txt RUN ansible-builder introspect --sanitize --user-pip=requirements.txt --write-bindep=/tmp/src/bindep.txt --write-pip=/tmp/src/requirements.txt RUN assemble FROM $EE_BASE_IMAGE USER root # Add our customizations here RUN mkdir /helper-playbooks COPY run-job-template.yml /helper-playbooks/ COPY --from=galaxy /usr/share/ansible /usr/share/ansible COPY --from=builder /output/ /output/ RUN /output/install-from-bindep &amp;&amp; rm -rf /output/wheels LABEL ansible-execution-environment=true&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;p&gt;We’ve added two steps: creating a directory and placing a playbook into it. This playbook is very simple and only acts as a “go-between” our pipeline and the Controller API, yet allows us to wait for jobs to complete and do a bit of validation of inputs:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: trigger job template run hosts: localhost gather_facts: false pre_tasks: - name: assert that vars are defined ansible.builtin.assert: that: - controller_hostname is defined - controller_username is defined - controller_password is defined - controller_validate_certs is defined - job_template is defined - name: set vars for role ansible.builtin.set_fact: controller_launch_jobs: - name: "{{ job_template }}" wait: true timeout: 14400 extra_vars: virtual_machine_name: "{{ virtual_machine_name | default('rhel9-vm') }}" resource_state: "{{ resource_state | default('present') }}" roles: - redhat_cop.controller_configuration.job_launch&lt;/code&gt;&lt;/pre&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;/div&gt; &lt;p&gt;Once the build is complete, this execution environment will also be consumable for our Device Edge build pipeline.&lt;/p&gt; &lt;h2&gt;Creating a pipeline to build Device Edge images&lt;/h2&gt; &lt;p&gt;With the automation pieces in place and an execution environment (container image) we can leverage as a simple interface between a pipeline and automation controller, we can start to build out a pipeline that will let us achieve our best practices for Device Edge images—defining them as code (IaC) and testing them before rolling them out to our fleet of devices (CI/CD).&lt;/p&gt; &lt;p&gt;From this point forward, we’re going to treat automation controller as what it is: a platform we can consume to run automation in the proper context and securely, all via the API.&lt;/p&gt; &lt;p&gt;The goal of our pipeline is to kick off a compose of a Device Edge image anytime we update or change our image definition. We’ll need to take some additional steps to set up for and capture our composed image, which the pipeline will also handle. Once those steps are completed, our pipeline will clean up all of the lingering pieces configured to ensure our compose works.&lt;/p&gt; &lt;p&gt;First, &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift 4.12&lt;/a&gt; includes a tech preview feature to manage virtual machines with OpenShift Pipelines, which allows us to easily spin up and spin down virtual machines as part of our pipeline.&lt;/p&gt; &lt;p&gt;Leveraging our customized execution environment from before, we’ll set up some tasks that will be strung together to form our pipeline. In addition, I’ve created a secret in the namespace of my virtual machine and pipeline that contains the details of my instance of Automation Controller; however, feel free to replace that with a proper secret storage system.&lt;/p&gt; &lt;p&gt;First, a task to expose the SSH port of the created virtual machine:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: manage-virtual-machine-connectivity namespace: image-builder spec: params: - name: virtualMachineName type: string description: The name of the virtual machine to expose default: rhel9-vm - name: resourceState type: string description: Creating or cleaning up default: present steps: - name: expose-virtual-machine image: quay.io/device-edge-workshops/helper-ee:latest env: - name: CONTROLLER_HOSTNAME valueFrom: secretKeyRef: name: controller-auth-account key: controller_hostname - name: CONTROLLER_USERNAME valueFrom: secretKeyRef: name: controller-auth-account key: controller_username - name: CONTROLLER_PASSWORD valueFrom: secretKeyRef: name: controller-auth-account key: controller_password - name: CONTROLLER_VALIDATE_CERTS valueFrom: secretKeyRef: name: controller-auth-account key: controller_validate_certs script: | ansible-playbook /helper-playbooks/run-job-template.yml \ --extra-vars "controller_hostname=$CONTROLLER_HOSTNAME" \ --extra-vars "controller_username=$CONTROLLER_USERNAME" \ --extra-vars "controller_password=$CONTROLLER_PASSWORD" \ --extra-vars "controller_validate_certs=$CONTROLLER_VALIDATE_CERTS" \ --extra-vars "job_template='Manage Virtual Machine Connectivity'" \ --extra-vars "virtual_machine_name=$(params.virtualMachineName)" \ --extra-vars "resource_state=$(params.resourceState)"&lt;/code&gt;&lt;/pre&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;/div&gt; &lt;p&gt;A good number of our tasks will look similar, so we can go through this task in detail and then simply make tweaks for later tasks.&lt;/p&gt; &lt;p&gt;From top to bottom, we’ve defined the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A name and namespace for the task.&lt;/li&gt; &lt;li&gt;Some parameters the task will take, and default values for them. Note that we’ve defined a parameter of &lt;code&gt;resourceState&lt;/code&gt;—this allows us to reuse this same task to both create and destroy resources, simply by feeding in a different value from the pipeline.&lt;/li&gt; &lt;li&gt;Inserting the values of our Kubernetes secret into the container environment.&lt;/li&gt; &lt;li&gt;Our execution environment we built earlier.&lt;/li&gt; &lt;li&gt;A simple script block that calls our helper playbook and feeds in the appropriate variables.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;When this task runs, the execution environment is started, ansible-playbook is invoked, and our corresponding variables are fed to the playbook, which communicates with the Controller API.&lt;/p&gt; &lt;p&gt;Our other tasks are similar, with minor tweaks to the &lt;code&gt;job_template&lt;/code&gt; variable so a different job template is called and executed by controller. As an added perk, the collection leveraged within our playbook will wait for controller to complete the job, then return success or failure accordingly, giving our pipeline the necessary visibility.&lt;/p&gt; &lt;p&gt;To view all the tasks, check out the &lt;code&gt;tasks&lt;/code&gt; directory on GitHub. You can create tasks using Ansible (similar to above, where we were configuring OpenShift) or the &lt;code&gt;oc&lt;/code&gt; CLI tool.&lt;/p&gt; &lt;p&gt;With our tasks created, we can build our pipeline:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: build-and-host-device-edge-image namespace: image-builder spec: tasks: - name: create-vm-from-template params: - name: templateName value: rhel9-image-builder-template - name: runStrategy value: RerunOnFailure - name: startVM value: 'true' taskRef: kind: ClusterTask name: create-vm-from-template - name: expose-virtual-machine-ssh params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) runAfter: - create-vm-from-template taskRef: kind: Task name: manage-virtual-machine-connectivity - name: create-host-in-controller params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) runAfter: - expose-virtual-machine-ssh taskRef: kind: Task name: manage-host-in-controller - name: preconfigure-virtual-machine params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) runAfter: - create-host-in-controller taskRef: kind: Task name: preconfigure-virtual-machine - name: install-image-builder params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) runAfter: - preconfigure-virtual-machine taskRef: kind: Task name: install-image-builder - name: expose-image-builder params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) runAfter: - install-image-builder taskRef: kind: Task name: manage-image-builder-connectivity - name: compose-image params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) runAfter: - install-image-builder - expose-image-builder taskRef: kind: Task name: compose-image - name: push-image-to-registry params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) runAfter: - compose-image taskRef: kind: Task name: push-image-to-registry - name: deploy-composed-image params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) runAfter: - push-image-to-registry taskRef: kind: Task name: push-image-to-registry finally: - name: cleanup-virtual-machine params: - name: vmName value: $(tasks.create-vm-from-template.results.name) - name: stop value: 'true' - name: delete value: 'true' taskRef: kind: ClusterTask name: cleanup-vm - name: cleanup-vm-connectivity params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) - name: resourceState value: absent taskRef: kind: Task name: manage-virtual-machine-connectivity - name: cleanup-image-builder-connectivity params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) - name: resourceState value: absent taskRef: kind: Task name: manage-image-builder-connectivity - name: cleanup-host-in-controller params: - name: virtualMachineName value: $(tasks.create-vm-from-template.results.name) - name: resourceState value: absent taskRef: kind: Task name: manage-host-in-controller&lt;/code&gt;&lt;/pre&gt; &lt;div class="highlight highlight-source-shell notranslate overflow-auto position-relative"&gt; &lt;/div&gt; &lt;p&gt;Let’s walk through the pipeline step-by-step:&lt;/p&gt; &lt;ol&gt;&lt;li dir="ltr"&gt;Create a virtual machine on OpenShift and pass the name to later tasks.&lt;/li&gt; &lt;li dir="ltr"&gt;Expose SSH to the virtual machine externally (this isn’t necessary, but it was useful while building and testing this process out).&lt;/li&gt; &lt;li dir="ltr"&gt;Create a corresponding host entry in automation controller.&lt;/li&gt; &lt;li dir="ltr"&gt;Run some preconfiguration steps on the virtual machine, such as registering to Red Hat Subscription Management.&lt;/li&gt; &lt;li dir="ltr"&gt;Install image builder.&lt;/li&gt; &lt;li dir="ltr"&gt;Compose a Device Edge image.&lt;/li&gt; &lt;li dir="ltr"&gt;Push the composed image to an image registry.&lt;/li&gt; &lt;li dir="ltr"&gt;Deploy the composed image to OpenShift.&lt;/li&gt; &lt;li dir="ltr"&gt;Clean up after ourselves.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;With this pipeline in place, we remove the burden of having to constantly run and manage a &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; image just to run image builder. Instead, all the infrastructure we need is spun up and down on demand, only existing while being consumed, then being destroyed after the work concludes.&lt;/p&gt; &lt;h2&gt;Expanding the concepts further&lt;/h2&gt; &lt;p&gt;This article is meant to serve as a foundation for building out an "edge manager in a box" capable of best practices for edge device management. As such, there are a few additional things we'd recommend adding to the above, but are out of scope for this specific tutorial:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Use a legitimate secret store:&lt;/strong&gt; There are a few places above where simple secret storage is used, and while functional, it is not at all recommended for production use cases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extending the pipeline:&lt;/strong&gt; Currently, the pipeline really only tests if the image will successfully build. Ideally, this would be extended to provision a "test" system using the new image, and test deploying edge applications onto it before declaring the whole process a success.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Image builder:&lt;/strong&gt; Eventually, we want image builder to operate in a container, even a privileged one, which eliminates the need for the virtualization aspects of this workflow.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Image Registry: &lt;/strong&gt;While the internal OpenShift Container Platform registry does work, using a scalable robust registry makes sense in production. For a primer, check out &lt;a href="https://cloud.redhat.com/blog/the-quintessential-red-hat-quay-quickstart"&gt;this blog post&lt;/a&gt; on getting started with Red Hat Quay.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Links&lt;/h2&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://www.ansible.com/blog/ansible-validated-content-introduction-to-infra.osbuild-collection"&gt;Infra.osbuild validated collection&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gregsowell.com/?p=7235"&gt;Ansible Controller As Code&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.12/operators/operator_sdk/ansible/osdk-ansible-k8s-collection.html"&gt;Kubernetes Ansible Collection&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/redhat-manufacturing/device-edge-demos"&gt;Device Edge Demos GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/redhat-manufacturing/device-edge-demos/tree/main/demos/rhde-pipeline"&gt;Red Hat Device Edge pipeline used in this article&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/30/build-edge-manager-single-node-openshift" title="Build an all-in-one edge manager with single-node OpenShift"&gt;Build an all-in-one edge manager with single-node OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Benjamin Schmaus, Josh Swanson</dc:creator><dc:date>2023-05-30T07:00:00Z</dc:date></entry><entry><title>How to use OpenShift GitOps to deploy applications</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/29/how-use-openshift-gitops-deploy-applications" /><author><name>Taiseer Hussein</name></author><id>4a01d27b-963d-45a3-829f-8260b8fb2496</id><updated>2023-05-29T07:00:00Z</updated><published>2023-05-29T07:00:00Z</published><summary type="html">&lt;p&gt;This article discusses how to use Red Hat OpenShift GitOps to automate application deployment. With a specific pattern, you will be able to deploy multiple applications to different environments once the app is committed to a Git repo. Follow this semi-guided demo to set up an automated method to deploy your application to &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; clusters using the &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; approach.&lt;/p&gt; &lt;p&gt;In this article, we will demonstrate the following processes:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Automating &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;continuous delivery&lt;/a&gt; with Red Hat OpenShift Pipelines and continuous deployment with OpenShift GitOps.&lt;/li&gt; &lt;li aria-level="1"&gt;Deploying an application to different environments.&lt;/li&gt; &lt;li aria-level="1"&gt;Promoting from lower to upper environments.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;The following prerequisites are required to replicate this demo:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Red Hat OpenShift 4.9 or later&lt;/li&gt; &lt;li aria-level="1"&gt;OpenShift GitOps Operator&lt;/li&gt; &lt;li aria-level="1"&gt;OpenShift Pipelines Operator&lt;/li&gt; &lt;li aria-level="1"&gt;Basic knowledge of OpenShift GitOps (Argo CD) applications and application sets.&lt;/li&gt; &lt;li aria-level="1"&gt;Basic knowledge of OpenShift Pipelines (Tekton).&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Our advice for using a GitOps approach&lt;/h2&gt; &lt;p&gt;The primary goal of the GitOps approach is to simplify the deployment of application workloads across multiple environments. This is accomplished using a single Git repository (monorepo) and organizing environments with a directory structure that supports flexibility and reuse. We also recommend using a mainline development approach with a single branch representing the source of truth for all environments. Use branches and merge requests to enable a testing, review, and approval workflow when propagating changes to various environments.&lt;/p&gt; &lt;p&gt;The Git repos for this demo:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://github.com/taiseerhussein/php-hello-dockerfile"&gt;Application source code&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://github.com/taiseerhussein/deploy-app-with-gitops.git"&gt;GitOps repo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;GitOps architecture&lt;/h2&gt; &lt;p&gt;The diagram in Figure 1 illustrates the architecture of the GitOps approach. To demonstrate the concepts, the different environments are represented as unique namespaces within a single cluster, not separate clusters. These namespaces mimic separate clusters from a conceptual perspective. GitHub, OpenShift Pipelines, Red Hat Quay, Kustomize, and OpenShift GitOps are the tools used in this approach.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/sf-dev.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/sf-dev.jpg?itok=iuLnVtFV" width="600" height="383" alt="Gitops architecture with the tools that have been used" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: An overview of high level GitOps architecture, listing the tools used.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;OpenShift Pipelines&lt;/h2&gt; &lt;p&gt;In this demonstration, we will model a simplistic continuous integration pipeline that clones an application source code repository, builds an application &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; image, pushes the resulting container image to an enterprise container registry (Quay), and updates a deployment manifest located in Git. We will implement these pipeline stages using common tasks that are available with OpenShift Pipelines. Finally, we will deploy the latest image to the cluster with continuous delivery using OpenShift GitOps.&lt;/p&gt; &lt;h2&gt;The directory structure provides flexibility&lt;/h2&gt; &lt;p&gt;This example GitOps approach utilizes a directory structure that allows for re-use across environments while also providing flexibility for customization. The directories are organized as follows.&lt;/p&gt; &lt;h3&gt;Application resources&lt;/h3&gt; &lt;p&gt;All of the applications resources such as deployments, ConfigMaps, routes, and services are located under the &lt;code&gt;org-services&lt;/code&gt; directory.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Common directory:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code&gt;common ├── config-base │   ├── configmap.yaml │   └── kustomization.yaml ├── manifest-base │   ├── base-deployment.yaml │   ├── base-deploymentorg.yaml │   ├── base-route.yaml │   ├── base-service.yaml │   └── kustomization.yaml ├── namespaces │   ├── kustomization.yaml │   ├── namespace1.yaml │   └── namespace2.yaml └── overlays ├── dev │   ├── configmap-1.yaml │   └── kustomization.yaml └── test ├── endpoints-common.yaml ├── kustomization.yaml └── test-configMap.yaml 6 directories, 15 files&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The common resources across all of the applications on all of the environments:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="2"&gt;&lt;strong&gt;config-base:&lt;/strong&gt; Contains the manifests for shared ConfigMaps across applications.&lt;/li&gt; &lt;li aria-level="2"&gt;&lt;strong&gt;manifest-base:&lt;/strong&gt; Contains the base of the other resources such as deployment, routes, and services.&lt;/li&gt; &lt;li aria-level="2"&gt;&lt;strong&gt;namespaces:&lt;/strong&gt; Contains the manifests the namespaces to be created.&lt;/li&gt; &lt;li aria-level="2"&gt;&lt;strong&gt;overlays:&lt;/strong&gt; Contains subdirectory for each environment.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;From each of these directories, OpenShift GitOps will create the shared ConfigMaps for each environment.&lt;/p&gt; &lt;p&gt;It’s important to note that we have separate directories for the ConfigMaps and the namespaces to avoid duplicate creation and conflicts since the name of these resources are the same across all of the application.&lt;/p&gt; &lt;p&gt;Under the services directory, create a subdirectory for each namespace. Then create another subdirectory for every service to be deployed under that specific namespace.&lt;/p&gt; &lt;p&gt;Each service has the following layout:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Base:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code&gt;php-hello/base ├── configmap.yaml └── kustomization.yaml 0 directories, 2 files&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This contains the shared resource for each specific service across all environments for a specific namespace. The &lt;code&gt;kustomization.yaml&lt;/code&gt; file under this directory patches the resources from the common directory by adding or updating the service specific fields and generates the YAML files.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt; 1 apiVersion: kustomize.config.k8s.io/v1beta1 2 kind: Kustomization 3 4 resources: 5 - configmap.yaml 6 -../../../../common/manifest-base/ 7 8 patches: 9 - patch: |- 10 - op: replace 11 path: /metadata/labels/app 12 value: php-hello 13 - op: replace 14 path: /metadata/name 15 value: php-hello 16 - op: replace 17 path: /spec/selector/matchLabels/app 18 value: php-hello 19 - op: replace 20 path: /spec/template/metadata/labels/app 21 value: php-hello 22 - op: replace 23 path: /spec/template/spec/containers/0/name 24 value: php-hello 25 target: 26 kind: Deployment 27 - patch: |- 28 - op: replace 29 path: /metadata/labels/app 30 value: php-hello 31 - op: replace 32 path: /metadata/name 33 value: php-hello 34 - op: replace 35 path: /spec/to/name 36 value: php-hello 37 target: 38 kind: Route 39 - patch: |- 40 - op: replace 41 path: /metadata/labels/app 42 value: php-hello 43 - op: replace 44 path: /metadata/name 45 value: php-hello 46 - op: replace 47 path: /spec/selector/app 48 value: php-hello 49 target: 50 kind: Service &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Overlay:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code&gt;overlays ├── dev-stable │   ├── configmap.yaml │   ├── kustomization.yaml │   ├── replicas.yaml │   └── version.yaml ├── dev-unstable │   ├── configmap.yaml │   ├── kustomization.yaml │   ├── replicas\ copy.yaml │   ├── replicas.yaml │   ├── version\ copy.yaml │   └── version.yaml ├── prod │   ├── configmap.yaml │   ├── kustomization.yaml │   ├── replicas.yaml │   └── version.yaml └── test ├── configmap.yaml ├── kustomization.yaml ├── replicas.yaml └── version.yaml 4 directories, 18 files&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The overlay contains a directory for each environment, which contains the patched resources for that specific environment. The Argo CD application points to this directory to deploy the application to a specific environment. The list of files and the contents could vary from one environment to another.&lt;/p&gt; &lt;p&gt;Let us take a look at the list of files under the &lt;code&gt;dev-stable&lt;/code&gt; environment directory to better understand this.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;code&gt;Configmap.yaml&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-yaml"&gt; 1 apiVersion: v1 2 data: 3 ENVIRONMENT: 'Development' 4 kind: ConfigMap 5 metadata: 6 name: php-hello&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This original file exists under the base directory. However, it is listed here since it needs another value for this specific environment. Kustomize patches and combines them to create a full environment specific ConfigMap.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;code&gt;Replicas.yaml&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To patch the deployment with the desired replicas for each environment:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt; 1 apiVersion: apps/v1 2 kind: Deployment 3 metadata: 4 name: php-hello 5 spec: 6 replicas: 1&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;code&gt;Version.yaml&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;This file gets updated by the continuous integration pipeline when a new container application image is created.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt; 1 apiVersion: apps/v1 2 kind: Deployment 3 metadata: 4 name: php-hello 5 spec: 6 template: 7 spec: 8 containers: 9 - image: quay.io/tkhussein/php-hello:2485c7e15c7ee4fe0ee15e4f71922d38144fd2e6 10 name: php-hello&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;ApplicationSet resources&lt;/h3&gt; &lt;p&gt;The ApplicationSet is the main driver for deploying the services to the cluster through Argo CD. The following are the directory structure and the ApplicationSets.&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;&lt;code&gt;kustomize/applicationsets/kustomize-appset-global&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt;&lt;pre&gt; &lt;code&gt;kustomize-appset-global ├── dev │   └── kustomization.yaml ├── stage │   └── kustomization.yaml └── test └── kustomization.yaml 3 directories, 3 files&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Each subdirectory in this directory is the entry point and the main driver for deploying all of the applications. To start deploying to each environment, you need to create a single Argo CD application that points to the directory corresponding to the environment. Once the Argo CD application is created, all of the services that belong to that environment will be automatically deployed.&lt;/p&gt; &lt;p&gt;This is a sample of the Kustomize YAML file that drives all of the deployments:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt; 1 apiVersion: kustomize.config.k8s.io/v1beta1 2 kind: Kustomization 3 4 resources: 5 -../../org-services/namespace1/overlays/dev 6 -../../org-services/namespace2/overlays/dev 7 8 # create namespaces appset. 9 -../../namespaces 10 # Create sealed secret appsets 11 -../../sealed-secrets/dev&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Lines 5 and 6 point to the ApplicationSets that creates applications for all services listed in that specific directory.&lt;/p&gt; &lt;p&gt;Line 9 is used to create the namespaces.&lt;/p&gt; &lt;p&gt;Line 11 creates sealed secrets.&lt;/p&gt; &lt;ol start="2"&gt;&lt;li aria-level="1"&gt;The &lt;code&gt;applicationsets/org-services&lt;/code&gt; directory contains a subdirectory for each namespace.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Within each namespace directory there are two ApplicationSets that are responsible for deploying shared resources as well as patching and deploying environment specific resources.&lt;/p&gt; &lt;p&gt;The Kustomize file within the overlays directories patches the ApplicationSet to include the details for that specific environment.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;org-services ├── namespace1 │   ├── base │   │   ├── common-appset.yaml │   │   ├── kustomization.yaml │   │   ├── php-hello-appset.yaml │   │   └── service2-appset.yaml │   └── overlays │   ├── dev │   │   └── kustomization.yaml │   └── stage │   └── kustomization.yaml └── namespace2 ├── base │   ├── babboon-appset.yaml │   ├── common-appset.yaml │   └── kustomization.yaml └── overlays ├── dev │   └── kustomization.yaml └── stage └── kustomization.yaml 10 directories, 11 files&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following is a sample Kustomize file under the &lt;code&gt;overlays&lt;/code&gt; directories:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt; 1 apiVersion: kustomize.config.k8s.io/v1beta1 2 kind: Kustomization 3 4 resources: 5 -../../base 6 7 patches: 8 - patch: |- 9 - op: add 10 path: /spec/generators/0/list 11 value: 12 elements: 13 - cluster: dev-stable 14 url: https://kubernetes.default.svc 15 namespace: namespace1 16 target: 17 kind: ApplicationSet 18 - patch: |- 19 - op: replace 20 path: /spec/template/spec/source/path 21 value: kustomize/org-services/common/overlays/dev 22 target: 23 name: php-hello-common&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This configuration is similar for the creation of services, namespaces, and sealed secrets.&lt;/p&gt; &lt;h3&gt;Sealed secrets&lt;/h3&gt; &lt;p&gt;Sealed secrets is a secure method to store encrypted secrets in Git and utilize a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; controller to deploy to a cluster. For more information about sealed secrets, check out the &lt;a href="https://github.com/bitnami-labs/sealed-secrets"&gt;source readme&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Onboarding a new application&lt;/h2&gt; &lt;p&gt;Once the directories and configurations for the service have been created, the following instructions describe how to deploy your service to the DEV cluster.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Log in to DEV cluster’s Argo CD instance. Then click on &lt;strong&gt;+ NEW APP&lt;/strong&gt; as shown in Figure 2.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/argocdnewapp_1.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/argocdnewapp_1.jpg?itok=YjAuxxfn" width="378" height="57" alt="Shows the Argo CD console and new app button." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: Creating a new ArgoCD application.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;Fill in the details, as shown in Figures 3 and 4.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fillargocdappinfo_1.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fillargocdappinfo_1.jpg?itok=u7XsDM8u" width="600" height="191" alt="A screenshot of the new Argo CD application form." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: The top part of the ArgoCD application form.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fillarogcdappinfo_2.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fillarogcdappinfo_2.jpg?itok=g5eUwA3J" width="600" height="303" alt="Shows a screenshot of the rest of the new Argo CD application form." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: The bottom part of the New ArgoCD application details.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The GitOps repo is the path to the Kustomize file that contains the starting point to deploy to DEV environment.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Verify the configuration you entered, then click &lt;strong&gt;CREATE&lt;/strong&gt;, as shown in Figure 5.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/clickcreatebutton.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/clickcreatebutton.jpg?itok=GSF1ybfm" width="234" height="57" alt="A screenshot of the Create button." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: Once the application details are verified, click the Create button.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Once the application is successfully created, it will automatically deploy the new services to DEV cluster. Figure 6 shows these apps.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/serviceapps_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/serviceapps_0.jpg?itok=IajWYk4o" width="600" height="284" alt="A screenshot of the list of all Argo CD applications." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: The list of Argo CD applications responsible for creating the resource for the deployments.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Updating the application&lt;/h2&gt; &lt;p&gt;If the changes are only on the source code, then the developers make their changes and commit them to their branch. This will trigger the pipeline line to build the new image and push it to Quay. Then the pipeline updates the following YAML file in the GitOps repository with the new image tag:&lt;/p&gt; &lt;p&gt;&lt;code&gt;kustomize/org-services/services/namespace1/php-hello/overlays/dev-stable/version.yaml&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Argo CD detects the changes, then it syncs the application and deploys new pods.&lt;/p&gt; &lt;p&gt;If the changes are on the app resources, such as number of replicas or ConfigMaps, then changes occur on the GitOps repo. Start with DEV, then move on into the upper environment. All the changes should be picked up by Argo CD automatically.&lt;/p&gt; &lt;h3&gt;Promoting across environments&lt;/h3&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;To promote a new application version, copy the version file from DEV to the higher environment and commit the changes.&lt;/li&gt; &lt;li aria-level="1"&gt;To promote resources in the common area, use an incremental process. For example, if there is a change in one of the ConfigMap shared across the environment, we make the changes on the overlay level. Once the test has been verified, it should be moved to the common area.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;GitOps simplifies application deployment&lt;/h2&gt; &lt;p&gt;The primary goal of the GitOps approach we demonstrated in this article is to simplify the deployment of application workloads across multiple environments. If you have questions, comment below. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/29/how-use-openshift-gitops-deploy-applications" title="How to use OpenShift GitOps to deploy applications"&gt;How to use OpenShift GitOps to deploy applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Taiseer Hussein</dc:creator><dc:date>2023-05-29T07:00:00Z</dc:date></entry><entry><title>3 patterns for deploying Helm charts with Argo CD</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/25/3-patterns-deploying-helm-charts-argocd" /><author><name>Trevor Royer</name></author><id>f29c38b8-072b-4a66-8451-8b36bd422412</id><updated>2023-05-25T07:00:00Z</updated><published>2023-05-25T07:00:00Z</published><summary type="html">&lt;p&gt;Argo CD provides numerous ways to deploy resources from a Helm chart. In this article, you will learn about three patterns used to manage and deploy Helm charts, including when and where to use each pattern in your &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; environment and the advantages and disadvantages.&lt;/p&gt; &lt;h2&gt;3 patterns for Helm charts&lt;/h2&gt; &lt;p&gt;We will discuss the following three patterns used to manage and deploy Helm charts:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Argo application pointing at a chart in a Helm repo.&lt;/li&gt; &lt;li&gt;Argo application pointing at a chart in a Git repo.&lt;/li&gt; &lt;li&gt;Argo application pointing at a Kustomize folder to render a chart.&lt;/li&gt; &lt;/ol&gt;&lt;h3 id="argo-application-pointing-at-a-chart-in-a-helm-repo"&gt;1. Argo application pointing at a chart in a Helm repo&lt;/h3&gt; &lt;p&gt;The first option for deploying a Helm chart is by referencing a chart that is hosted in a Helm repository.&lt;/p&gt; &lt;p&gt;When deploying a chart from the Argo CD UI, users provide a URL to the Helm repo containing a collection of charts and selects the &lt;strong&gt;Helm&lt;/strong&gt; option in the &lt;strong&gt;Source&lt;/strong&gt; menu. The &lt;strong&gt;Chart&lt;/strong&gt; and &lt;strong&gt;Version&lt;/strong&gt; fields will provide a list of available options from a dropdown menu (Figure 1).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/argo-helm-repo-ui.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/argo-helm-repo-ui.png?itok=6wEMB99e" width="600" height="133" alt="A screenshot of the Argo CD Helm repo configuration form." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The ArgoCD Helm repo configuration page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once you have entered the a chart in the &lt;strong&gt;Source&lt;/strong&gt; section, a &lt;strong&gt;Helm&lt;/strong&gt; section will become available, allowing you to specify a values file, values in a YAML format, or the default parameters auto-populated by the chart (Figure 2).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/argo-helm-values-ui.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/argo-helm-values-ui.png?itok=bGtjDXZw" width="600" height="303" alt="A screenshot of the Argo CD Helm parameters configuration form." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The ArgoCD Helm parameters configuration page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4 id="advantages"&gt;Advantages and disadvantages of deploying a chart from a Helm repo&lt;/h4&gt; &lt;p&gt;The advantage of deploying a chart directly from a Helm repo is that the UI provides a simple and intuitive user experience. The UI auto-populates the default parameters, presenting configurable options to end users and avoiding mistakes such as misspelled parameter names. This ease of use makes this one of the first options for new users of Argo.&lt;/p&gt; &lt;p&gt;However, this option makes it challenging to troubleshoot or render a helm chart from a development machine with the &lt;code&gt;helm template&lt;/code&gt; command. Any parameters that are populated in the UI are added into the Argo application object which can be manually duplicated on the command line when running &lt;code&gt;helm template&lt;/code&gt;. But this option leaves room for errors and typos. The future option to add a &lt;code&gt;values.yaml&lt;/code&gt; file from a separate Git repo greatly improves the ability to render the chart locally, but it can leave the &lt;code&gt;values.yaml&lt;/code&gt; file orphaned in the Git repo without any additional context, such as the chart repo, name, and version.&lt;/p&gt; &lt;p&gt;Another disadvantage is that this design pattern does not allow for any flexibility or customization to objects deployed in the chart that are not explicitly allowed by the original chart author. For example, if the original author does not include options to set a &lt;code&gt;nodeSelector&lt;/code&gt; in the values, users will not have the ability to set that option in a &lt;code&gt;deployment&lt;/code&gt;.&lt;/p&gt; &lt;h4 id="other-considerations"&gt;Other considerations&lt;/h4&gt; &lt;p&gt;Deploying the chart directly from a Helm repo is best for deploying charts that are well maintained, documented, and require minimal troubleshooting. This option is great for rapid chart deployment and prototyping or set-it-and-forget-it deployments.&lt;/p&gt; &lt;p&gt;The challenges of rendering the chart locally can make this option especially challenging when developing custom charts. In many cases, too much logic and configuration ends up in the Argo application object making it difficult to maintain. This feature in Argo does not currently allow you to utilize another Git repo as a source for the &lt;code&gt;values.yaml&lt;/code&gt; file, which is one of the main challenges of using this pattern for resources that need to be maintained over time.&lt;/p&gt; &lt;h3 id="argo-application-pointing-at-a-chart-in-a-git-repo"&gt;2. Argo application pointing at a chart in a Git repo&lt;/h3&gt; &lt;p&gt;Another option for deploying a Helm chart with Argo is generating a chart and storing it directly in a Git repo. When using this option, users provide a Git repo URL and the path to the &lt;code&gt;Chart.yaml&lt;/code&gt; file. Argo will automatically detect the Helm chart and render the chart when deploying.&lt;/p&gt; &lt;p&gt;Charts stored in the Git repo can be a fully self-contained chart with their own yaml templates or it can take advantage of chart dependencies to deploy charts hosted in a Helm repo or another chart in the same Git repo. Utilizing a chart to configure a dependency and setting parameters with the values.yaml file of that chart are sometimes referred to as a proxy chart.&lt;/p&gt; &lt;p&gt;To utilize a chart stored in a Helm repo, you can provide the dependency information in the &lt;code&gt;Chart.yaml&lt;/code&gt; object as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-yaml"&gt;&lt;span class="hljs-attribute"&gt;dependencies&lt;/span&gt;: - &lt;span class="hljs-attribute"&gt;name&lt;/span&gt;: &lt;span class="hljs-string"&gt;"mlflow-server"&lt;/span&gt; &lt;span class="hljs-attribute"&gt;version&lt;/span&gt;: &lt;span class="hljs-string"&gt;"0.5.7"&lt;/span&gt; &lt;span class="hljs-attribute"&gt;repository&lt;/span&gt;: &lt;span class="hljs-string"&gt;"https://strangiato.github.io/helm-charts/"&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To reference another chart located in the same Git repo, you can utilize the &lt;code&gt;file://&lt;/code&gt; protocol in the &lt;code&gt;Chart.yaml&lt;/code&gt; files repository field:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-yaml"&gt;&lt;span class="hljs-attribute"&gt;dependencies&lt;/span&gt;: - &lt;span class="hljs-attribute"&gt;name&lt;/span&gt;: &lt;span class="hljs-string"&gt;"my-local-chart"&lt;/span&gt; &lt;span class="hljs-attribute"&gt;version&lt;/span&gt;: &lt;span class="hljs-string"&gt;"0.1.0"&lt;/span&gt; &lt;span class="hljs-attribute"&gt;repository&lt;/span&gt;: &lt;span class="hljs-string"&gt;"file://../my-local-chart/"&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can configure parameters in the local Helm chart by using the &lt;code&gt;values.yaml&lt;/code&gt; file, and Argo will automatically utilize this file when rendering the chart.&lt;/p&gt; &lt;p&gt;Leveraging chart dependencies within the same Git repo allows for a flexible pattern for building out a multi-tiered application deployment to different environments. By creating a simple chart folder structure, such as the following example, users can develop a custom chart for an application deployed to multiple environments and provide configuration differences in the environment-charts &lt;code&gt;values.yaml&lt;/code&gt; file.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;. ├── common-charts │ └── &lt;span class="hljs-keyword"&gt;my&lt;/span&gt;-&lt;span class="hljs-built_in"&gt;application&lt;/span&gt; └── environment-charts ├── dev │ └── &lt;span class="hljs-keyword"&gt;my&lt;/span&gt;-&lt;span class="hljs-built_in"&gt;application&lt;/span&gt; ├── prod │ └── &lt;span class="hljs-keyword"&gt;my&lt;/span&gt;-&lt;span class="hljs-built_in"&gt;application&lt;/span&gt; └── test └── &lt;span class="hljs-keyword"&gt;my&lt;/span&gt;-&lt;span class="hljs-built_in"&gt;application&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt; &lt;h4 id="advantages"&gt;Advantages and disadvantages of deploying a chart from a Git repo&lt;/h4&gt; &lt;p&gt;An advantage of this design pattern provides the most native Helm developer experience and allows developers to take advantage of Helm features, such as &lt;code&gt;helm template&lt;/code&gt; and &lt;code&gt;helm lint&lt;/code&gt; in their local environment, allowing them to easily render the chart locally for testing.&lt;/p&gt; &lt;p&gt;Another advantage of this pattern is when deploying to multiple environments, it enables you to manage the lifecycle of your chart separately in each environment. When utilizing a dependency of a chart stored in a Helm repo, your dev environment can be utilizing &lt;code&gt;v1.1.0&lt;/code&gt; while your prod environment is utilizing &lt;code&gt;v1.0.0&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;A disadvantage of deploying a chart from a Git repo is similar to the Helm repo pattern. If the original author does not provide an option to configure a specific setting, users will not have the ability to set those options.&lt;/p&gt; &lt;p&gt;This option is also limited to only allowing users to provide parameters in the &lt;code&gt;values.yaml&lt;/code&gt; file. Users are not able to create separate &lt;code&gt;values.yaml&lt;/code&gt; files for different environments in a single chart and instead must create a separate chart for each environment they wish to configure.&lt;/p&gt; &lt;p&gt;Another disadvantage is that this pattern can create junk files for a simple deployment that may not be necessary in the final Git repo, such as &lt;code&gt;.helmignore&lt;/code&gt;, &lt;code&gt;Chart.lock&lt;/code&gt; or dependent chart &lt;code&gt;*.tgz&lt;/code&gt; files downloaded locally for testing. Some of these files may be added to the &lt;code&gt;.gitignore&lt;/code&gt; file to reduce clutter in the repo.&lt;/p&gt; &lt;h4 id="other-considerations"&gt;Other considerations&lt;/h4&gt; &lt;p&gt;This option is ideal for getting maximum flexibility when developing a custom charts. The ability to create a simple chart without packaging and storing it in a Helm repo allows for extremely rapid prototyping.&lt;/p&gt; &lt;p&gt;If you manage a chart with a more complex lifecycle, this pattern allows users to maintain different environments with different chart versions and promote changes through the environments in a similar way that images can be promoted to different environments.&lt;/p&gt; &lt;h3 id="argo-application-pointing-at-a-kustomize-overlay-rendering-a-chart"&gt;3. Argo application pointing at a Kustomize folder to render a chart&lt;/h3&gt; &lt;p&gt;The third pattern for deploying Helm charts with Argo is by rendering a Helm chart with Kustomize. In your &lt;code&gt;kustomization.yaml&lt;/code&gt; file, you can provide chart details, including the Helm repo, chart version, and values. This provides similar capabilities to the proxy chart capabilities with the Kustomize tooling.&lt;/p&gt; &lt;p&gt;Values can be provided using &lt;code&gt;valuesFile&lt;/code&gt; to reference a file relative to the &lt;code&gt;kustomization.yaml&lt;/code&gt; file or with &lt;code&gt;valuesInline&lt;/code&gt; where you can directly specify parameters.&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-yaml"&gt;&lt;span class="hljs-attribute"&gt;apiVersion&lt;/span&gt;: kustomize.config.k8s.io/v1beta1 &lt;span class="hljs-attribute"&gt;kind&lt;/span&gt;: Kustomization &lt;span class="less"&gt;&lt;span class="hljs-attribute"&gt;helmCharts&lt;/span&gt;: - &lt;span class="hljs-attribute"&gt;name&lt;/span&gt;: mlflow-server &lt;span class="hljs-attribute"&gt;repo&lt;/span&gt;: &lt;span class="hljs-attribute"&gt;https&lt;/span&gt;:&lt;span class="hljs-comment"&gt;//strangiato.github.io/helm-charts/&lt;/span&gt; &lt;span class="hljs-attribute"&gt;version&lt;/span&gt;: &lt;span class="hljs-string"&gt;"0.5.7"&lt;/span&gt; &lt;span class="hljs-attribute"&gt;releaseName&lt;/span&gt;: mlflow-server &lt;span class="hljs-attribute"&gt;namespace&lt;/span&gt;: my-namespace &lt;span class="hljs-attribute"&gt;valuesFile&lt;/span&gt;: values.yaml &lt;span class="hljs-attribute"&gt;valuesInline&lt;/span&gt;: &lt;span class="hljs-attribute"&gt;fullnameOverride&lt;/span&gt;: helloagain&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;From your local environment, you can render the chart by running &lt;code&gt;kustomize build . --enable-helm&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;To utilize this option with Argo, you must provide the &lt;code&gt;enable-helm&lt;/code&gt; flag in the Argo CD object definition as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-yaml"&gt;&lt;span class="hljs-symbol"&gt;apiVersion:&lt;/span&gt; argoproj.io/v1alpha1 &lt;span class="hljs-symbol"&gt;kind:&lt;/span&gt; ArgoCD &lt;span class="hljs-symbol"&gt;metadata:&lt;/span&gt; &lt;span class="hljs-symbol"&gt; name:&lt;/span&gt; argocd &lt;span class="hljs-symbol"&gt;spec:&lt;/span&gt; &lt;span class="hljs-symbol"&gt; kustomizeBuildOptions:&lt;/span&gt; &lt;span class="hljs-string"&gt;"--enable-helm"&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt; &lt;h4 id="advantages"&gt;Advantages and disadvantages of rendering a Helm chart with Kustomize&lt;/h4&gt; &lt;p&gt;If a team is already heavily relying on Kustomize in their GitOps environments, utilizing Kustomize to render a Helm chart can help to keep a higher consistency with other configurations and reduce the number of tools needed in the repo.&lt;/p&gt; &lt;p&gt;Another advantage is that the combination of Kustomize with Helm also provides a powerful option to patch objects. When leveraging the base/overlays Kustomize pattern, a Helm chart renders in the base layer and additional patches apply in overlays. The ability to apply patches after the Helm chart renders allows you to modify the objects in ways the original chart author did not include.&lt;/p&gt; &lt;p&gt;A disadvantage is that the &lt;code&gt;--enable-helm&lt;/code&gt; flag introduces complexity when attempting to troubleshoot a chart locally. Users may also experience issues when attempting to apply the Kustomize resources with &lt;code&gt;oc apply -k&lt;/code&gt; since the Kustomize tools built into &lt;code&gt;oc&lt;/code&gt;/&lt;code&gt;kubectl&lt;/code&gt; do not support the &lt;code&gt;--enable-helm&lt;/code&gt; flag. Additionally, this option does require modification to the default Argo CD deployment to enable the feature, which some users may not have permission to do.&lt;/p&gt; &lt;p&gt;Another disadvantage when using this pattern, is that once Kustomize has inflated the chart, the objects are treated just like any other yaml objects, and is no longer Helm chart. When utilizing the base/overlays model as previously described, you will lose the ability to control the chart objects using the values parameters.&lt;/p&gt; &lt;h4 id="other-considerations"&gt;Other considerations&lt;/h4&gt; &lt;p&gt;This option is ideal for users that are already heavily relying on Kustomize and don't want to introduce another tool their environment. This option is also fantastic when you do not control the Helm chart that you are attempting to deploy, and you need to modify it in a way that the original author didn't include as a configurable option.&lt;/p&gt; &lt;h2 id="final-thoughts"&gt;Helping you choose a pattern for Helm chart deployment&lt;/h2&gt; &lt;p&gt;In future versions of &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift GitOps&lt;/a&gt;, Argo CD will support the ability to define multiple sources for objects, such as a Helm chart from one repo and a &lt;code&gt;values.yaml&lt;/code&gt; file from another, which could help to eliminate some of the shortcomings of deploying a Helm chart directly from a Helm repo. This feature is discussed in more detail in the article &lt;a href="https://developers.redhat.com/articles/2023/02/20/multiple-sources-argo-cd-applications#"&gt;Multiple sources for Argo CD applications&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;One of the major challenges faced by the GitOps community is finding the correct way to manage resources and a GitOps repo with growing complexity. In many cases, there is no one correct solution, and the three options presented here are valid patterns for deploying and managing Helm charts. Hopefully, the advantages and disadvantages discussed in this article provided insight for the next time you need to choose the best option to incorporate a Helm chart into your environment.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/25/3-patterns-deploying-helm-charts-argocd" title="3 patterns for deploying Helm charts with Argo CD"&gt;3 patterns for deploying Helm charts with Argo CD&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Trevor Royer</dc:creator><dc:date>2023-05-25T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 3.0.4.Final released - Maintenance release</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-3-0-4-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-3-0-4-final-released/</id><updated>2023-05-25T00:00:00Z</updated><content type="html">We released Quarkus 3.0.4.Final, the third maintenance release of our 3.0 release train (as our first public release for 3.0 was 3.0.1.Final). As usual, it contains bugfixes and documentation improvements. It should be a safe upgrade for anyone already using 3.0. If you are not already using 3.0, please refer...</content><dc:creator>Guillaume Smet</dc:creator></entry></feed>
